{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RoBERTa with Fastai - CB Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "\n",
    "Dev Sharma, Using RoBERTa with fast.ai for SuperGLUE Task CB\n",
    "Finetuning state of the art RoBERTa with fast.ai on Commitment Bank NLP task.\n",
    "\n",
    "This notebook follows the tutorial @ https://medium.com/@devkosal/superglue-roberta-with-fastai-for-rte-task-c362961be957\n",
    "\n",
    "Related article:\n",
    "\n",
    "Using RoBERTa with fast.ai for NLP\n",
    "Implementing the current state of the art NLP model in fast.ai\n",
    "\n",
    "https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T02:30:58.738741Z",
     "start_time": "2019-09-04T02:30:57.798726Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.metrics import *\n",
    "from fastai.text.data import *\n",
    "from fastai.text.all import *\n",
    "\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T02:30:58.744883Z",
     "start_time": "2019-09-04T02:30:58.740327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a config object to store task specific information\n",
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "        \n",
    "config = Config(\n",
    "    task = \"CB\",\n",
    "    testing=False,\n",
    "    seed = 2019,\n",
    "    roberta_model_name='roberta-base', # can also be exchanged with roberta-large \n",
    "    max_lr=1e-5,\n",
    "    epochs=10,\n",
    "    use_fp16=False,\n",
    "    bs=4, \n",
    "    max_seq_len=256, \n",
    "    num_labels = 3,\n",
    "    hidden_dropout_prob=.05,\n",
    "    hidden_size=768, # 1024 for roberta-large\n",
    "    start_tok = \"<s>\",\n",
    "    end_tok = \"</s>\",\n",
    "    mark_fields=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T02:31:00.072931Z",
     "start_time": "2019-09-04T02:31:00.050079Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(\"./superglue_data/CB/train.jsonl\",lines=True)\n",
    "val = pd.read_json(\"./superglue_data/CB/val.jsonl\",lines=True)\n",
    "test = pd.read_json(\"./superglue_data/CB/test.jsonl\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.053988Z",
     "start_time": "2019-09-03T23:52:36.049310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 3)\n"
     ]
    }
   ],
   "source": [
    "# drop the unnecessary idx column\n",
    "for df in (train,val):\n",
    "    if \"idx\" in df.columns: df.drop(\"idx\",axis=1,inplace=True)\n",
    "        \n",
    "if config.testing:\n",
    "    train = train[:100]\n",
    "    val = val[:100]\n",
    "    \n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.067099Z",
     "start_time": "2019-09-03T23:52:36.055142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It was a complex language. Not written down but handed down. One might say it was peeled down.</td>\n",
       "      <td>the language was peeled down</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It is part of their religion, a religion I do not scoff at as it holds many elements which match our own even though it lacks the truth of ours. At one of their great festivals they have the ritual of driving out the devils from their bodies. First the drummers come on - I may say that no women are allowed to take part in this ritual and the ladies here will perhaps agree with me that they are fortunate in that omission.</td>\n",
       "      <td>no women are allowed to take part in this ritual</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Paris to Rouen railway was being extended to Le Havre, and the line cut straight through Dr Flaubert's land. Part of it was to be compulsorily purchased. You could say that Gustave was shepherded into creative retreat at Croisset by epilepsy.</td>\n",
       "      <td>Gustave was shepherded into creative retreat at Croisset by epilepsy</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Part of it was to be compulsorily purchased. You could say that Gustave was shepherded into creative retreat at Croisset by epilepsy. You could also say he was driven there by the railway.</td>\n",
       "      <td>Gustave was driven to creative retreat in Croisset by the railway</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Some of them, like for instance the farm in Connecticut, are quite small. If I like a place I buy it. I guess you could say it's a hobby.</td>\n",
       "      <td>buying places is a hobby</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                    premise  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                            It was a complex language. Not written down but handed down. One might say it was peeled down.   \n",
       "1  It is part of their religion, a religion I do not scoff at as it holds many elements which match our own even though it lacks the truth of ours. At one of their great festivals they have the ritual of driving out the devils from their bodies. First the drummers come on - I may say that no women are allowed to take part in this ritual and the ladies here will perhaps agree with me that they are fortunate in that omission.   \n",
       "2                                                                                                                                                                                    The Paris to Rouen railway was being extended to Le Havre, and the line cut straight through Dr Flaubert's land. Part of it was to be compulsorily purchased. You could say that Gustave was shepherded into creative retreat at Croisset by epilepsy.   \n",
       "3                                                                                                                                                                                                                                              Part of it was to be compulsorily purchased. You could say that Gustave was shepherded into creative retreat at Croisset by epilepsy. You could also say he was driven there by the railway.   \n",
       "4                                                                                                                                                                                                                                                                                                 Some of them, like for instance the farm in Connecticut, are quite small. If I like a place I buy it. I guess you could say it's a hobby.   \n",
       "\n",
       "                                                             hypothesis  \\\n",
       "0                                          the language was peeled down   \n",
       "1                      no women are allowed to take part in this ritual   \n",
       "2  Gustave was shepherded into creative retreat at Croisset by epilepsy   \n",
       "3     Gustave was driven to creative retreat in Croisset by the railway   \n",
       "4                                              buying places is a hobby   \n",
       "\n",
       "        label  \n",
       "0  entailment  \n",
       "1  entailment  \n",
       "2  entailment  \n",
       "3  entailment  \n",
       "4  entailment  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.077736Z",
     "start_time": "2019-09-03T23:52:36.068230Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contradiction    119\n",
       "entailment       115\n",
       "neutral           16\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.086245Z",
     "start_time": "2019-09-03T23:52:36.079426Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_cols = [\"premise\",\"hypothesis\"]\n",
    "label_cols = \"label\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.093982Z",
     "start_time": "2019-09-03T23:52:36.087757Z"
    }
   },
   "outputs": [],
   "source": [
    "class FastAiRobertaTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n",
    "    def __init__(self, tokenizer: RobertaTokenizer, max_seq_len: int=128, **kwargs): \n",
    "        self._pretrained_tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len \n",
    "    def __call__(self, *args, **kwargs): \n",
    "        return self \n",
    "    def tokenizer(self, t:str) -> List[str]: \n",
    "        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n",
    "        if config.mark_fields:\n",
    "            sub = 2 # subtraction in totoal seq_length to be made due to adding spcl tokens\n",
    "            assert \"xxfld\" in t\n",
    "            t = t.replace(\"xxfld 1\",\"\") # remove the xxfld 1 special token from fastai\n",
    "            # converting fastai field sep token to Roberta\n",
    "            t = re.split(r'xxfld \\d+', t) \n",
    "            res = []\n",
    "            for i in range(len(t)-1): # loop over the number of additional fields and the Roberta sep\n",
    "                res += self._pretrained_tokenizer.tokenize(t[i]) + [config.end_tok, config.end_tok]\n",
    "                sub += 2 # increase our subtractions since we added more spcl tokens\n",
    "            res += self._pretrained_tokenizer.tokenize(t[-1]) # add the last sequence\n",
    "            return [config.start_tok] + res[:self.max_seq_len - sub] + [config.end_tok] \n",
    "        \n",
    "        res = self._pretrained_tokenizer.tokenize(t)\n",
    "        return [config.start_tok] + res[:self.max_seq_len - sub] + [config.end_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.427223Z",
     "start_time": "2019-09-03T23:52:36.095149Z"
    }
   },
   "outputs": [],
   "source": [
    "# create fastai tokenizer for roberta\n",
    "# source: https://docs.fast.ai/text.core.html#tokenizer-\n",
    "\n",
    "roberta_tok = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "fastai_tokenizer = Tokenizer(tok=FastAiRobertaTokenizer(\n",
    "    roberta_tok, max_seq_len=config.max_seq_len), rules=[]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.549924Z",
     "start_time": "2019-09-03T23:52:36.428608Z"
    }
   },
   "outputs": [],
   "source": [
    "# create fastai vocabulary for roberta    \n",
    "path=\"./output\"\n",
    "roberta_tok.save_vocabulary(path)\n",
    "\n",
    "with open('./output/vocab.json', 'r') as f:\n",
    "    roberta_vocab_dict = json.load(f)\n",
    "    \n",
    "# fastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys()))\n",
    "fastai_roberta_vocab = list(roberta_vocab_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.555292Z",
     "start_time": "2019-09-03T23:52:36.551292Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TokenizeProcessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Setting up pre-processors\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mall\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRobertaTokenizeProcessor\u001b[39;00m(\u001b[43mTokenizeProcessor\u001b[49m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenizer):\n\u001b[1;32m      6\u001b[0m          \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, include_bos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, include_eos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, mark_fields\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmark_fields)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TokenizeProcessor' is not defined"
     ]
    }
   ],
   "source": [
    "# Setting up pre-processors\n",
    "class RobertaTokenizeProcessor(TokenizeProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "         super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False, mark_fields=config.mark_fields)\n",
    "\n",
    "class RobertaNumericalizeProcessor(NumericalizeProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, vocab=fastai_roberta_vocab, **kwargs)\n",
    "\n",
    "\n",
    "def get_roberta_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
    "    \"\"\"\n",
    "    Constructing preprocessors for Roberta\n",
    "    We remove sos and eos tokens since we add that ourselves in the tokenizer.\n",
    "    We also use a custom vocabulary to match the numericalization with the original Roberta model.\n",
    "    \"\"\"\n",
    "    return [RobertaTokenizeProcessor(tokenizer=tokenizer), NumericalizeProcessor(vocab=vocab)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.563667Z",
     "start_time": "2019-09-03T23:52:36.556702Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextDataBunch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Creating a Roberta specific DataBunch class\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRobertaDataBunch\u001b[39;00m(\u001b[43mTextDataBunch\u001b[49m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate a `TextDataBunch` suitable for training Roberta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\u001b[38;5;28mcls\u001b[39m, train_ds, valid_ds, test_ds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, path:PathOrStr\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, bs:\u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, val_bs:\u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pad_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      6\u001b[0m                pad_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device:torch\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, no_check:\u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, backwards:\u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m      7\u001b[0m                dl_tfms:Optional[Collection[Callable]]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdl_kwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataBunch:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TextDataBunch' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating a Roberta specific DataBunch class\n",
    "class RobertaDataBunch(TextDataBunch):\n",
    "    \"Create a `TextDataBunch` suitable for training Roberta\"\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=64, val_bs:int=None, pad_idx=1,\n",
    "               pad_first=True, device:torch.device=None, no_check:bool=False, backwards:bool=False, \n",
    "               dl_tfms:Optional[Collection[Callable]]=None, **dl_kwargs) -> DataBunch:\n",
    "        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        val_bs = ifnone(val_bs, bs)\n",
    "        collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
    "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs)\n",
    "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
    "        dataloaders = [train_dl]\n",
    "        for ds in datasets[1:]:\n",
    "            lengths = [len(t) for t in ds.x.items]\n",
    "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
    "        return cls(*dataloaders, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.570550Z",
     "start_time": "2019-09-03T23:52:36.565011Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRobertaTextList\u001b[39;00m(\u001b[43mTextList\u001b[49m):\n\u001b[1;32m      2\u001b[0m     _bunch \u001b[38;5;241m=\u001b[39m RobertaDataBunch\n\u001b[1;32m      3\u001b[0m     _label_cls \u001b[38;5;241m=\u001b[39m TextList\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TextList' is not defined"
     ]
    }
   ],
   "source": [
    "class RobertaTextList(TextList):\n",
    "    _bunch = RobertaDataBunch\n",
    "    _label_cls = TextList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:38.226583Z",
     "start_time": "2019-09-03T23:52:36.571900Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_roberta_processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# loading the tokenizer and vocab processors\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mget_roberta_processor\u001b[49m(tokenizer\u001b[38;5;241m=\u001b[39mfastai_tokenizer, vocab\u001b[38;5;241m=\u001b[39mfastai_roberta_vocab)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# creating our databunch \u001b[39;00m\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m ItemLists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, RobertaTextList\u001b[38;5;241m.\u001b[39mfrom_df(train, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, cols\u001b[38;5;241m=\u001b[39mfeat_cols, processor\u001b[38;5;241m=\u001b[39mprocessor),\n\u001b[1;32m      6\u001b[0m                       RobertaTextList\u001b[38;5;241m.\u001b[39mfrom_df(val, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, cols\u001b[38;5;241m=\u001b[39mfeat_cols, processor\u001b[38;5;241m=\u001b[39mprocessor)\n\u001b[1;32m      7\u001b[0m                 ) \\\n\u001b[1;32m      8\u001b[0m        \u001b[38;5;241m.\u001b[39mlabel_from_df(cols\u001b[38;5;241m=\u001b[39mlabel_cols, label_cls\u001b[38;5;241m=\u001b[39mCategoryList) \\\n\u001b[1;32m      9\u001b[0m        \u001b[38;5;241m.\u001b[39madd_test(RobertaTextList\u001b[38;5;241m.\u001b[39mfrom_df(test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, cols\u001b[38;5;241m=\u001b[39mfeat_cols, processor\u001b[38;5;241m=\u001b[39mprocessor)) \\\n\u001b[1;32m     10\u001b[0m        \u001b[38;5;241m.\u001b[39mdatabunch(bs\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbs,pad_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_roberta_processor' is not defined"
     ]
    }
   ],
   "source": [
    "# loading the tokenizer and vocab processors\n",
    "processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)\n",
    "\n",
    "# creating our databunch \n",
    "data = ItemLists(\".\", RobertaTextList.from_df(train, \".\", cols=feat_cols, processor=processor),\n",
    "                      RobertaTextList.from_df(val, \".\", cols=feat_cols, processor=processor)\n",
    "                ) \\\n",
    "       .label_from_df(cols=label_cols, label_cls=CategoryList) \\\n",
    "       .add_test(RobertaTextList.from_df(test, \".\", cols=feat_cols, processor=processor)) \\\n",
    "       .databunch(bs=config.bs,pad_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:38.232898Z",
     "start_time": "2019-09-03T23:52:38.228130Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "# defining our model architecture \n",
    "class RobertaForSequenceClassificationModel(nn.Module):\n",
    "    def __init__(self,num_labels=config.num_labels):\n",
    "        super(RobertaForSequenceClassificationModel,self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta = RobertaForSequenceClassification.from_pretrained(config.roberta_model_name,num_labels= self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(input_ids, token_type_ids, attention_mask)\n",
    "        logits = outputs[0] \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:41.690696Z",
     "start_time": "2019-09-03T23:52:38.234218Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'fastai.text.data' has no attribute 'train_ds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m roberta_model \u001b[38;5;241m=\u001b[39m RobertaForSequenceClassificationModel() \n\u001b[0;32m----> 3\u001b[0m learn \u001b[38;5;241m=\u001b[39m \u001b[43mLearner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroberta_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43maccuracy\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py:121\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, dls, model, loss_func, opt_func, lr, splitter, cbs, metrics, path, model_dir, wd, wd_bn_bias, train_bn, moms, default_cbs)\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'fastai.text.data' has no attribute 'train_ds'"
     ]
    }
   ],
   "source": [
    "roberta_model = RobertaForSequenceClassificationModel() \n",
    "\n",
    "learn = Learner(data, roberta_model, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:54:57.085222Z",
     "start_time": "2019-09-03T23:52:41.692566Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.model.roberta.train() # setting roberta to train as it is in eval mode by default\n",
    "learn.fit_one_cycle(config.epochs, max_lr=config.max_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:54:57.090526Z",
     "start_time": "2019-09-03T23:54:57.086949Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
    "    learn.model.roberta.eval()\n",
    "    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n",
    "    sampler = [i for i in data.dl(ds_type).sampler]\n",
    "    reverse_sampler = np.argsort(sampler)\n",
    "    ordered_preds = preds[reverse_sampler, :]\n",
    "    pred_values = np.argmax(ordered_preds, axis=1)\n",
    "    return ordered_preds, pred_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:54:57.934272Z",
     "start_time": "2019-09-03T23:54:57.091801Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DatasetType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# val preds\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m preds, pred_values \u001b[38;5;241m=\u001b[39m get_preds_as_nparray(\u001b[43mDatasetType\u001b[49m\u001b[38;5;241m.\u001b[39mValid)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DatasetType' is not defined"
     ]
    }
   ],
   "source": [
    "# val preds\n",
    "preds, pred_values = get_preds_as_nparray(DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:54:57.939243Z",
     "start_time": "2019-09-03T23:54:57.935988Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# accuracy for valid valid\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m (\u001b[43mpred_values\u001b[49m \u001b[38;5;241m==\u001b[39m data\u001b[38;5;241m.\u001b[39mvalid_ds\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mitems)\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_values' is not defined"
     ]
    }
   ],
   "source": [
    "# accuracy for valid valid\n",
    "(pred_values == data.valid_ds.y.items).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:55:00.674531Z",
     "start_time": "2019-09-03T23:54:57.940330Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DatasetType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test preds\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m _, test_pred_values \u001b[38;5;241m=\u001b[39m get_preds_as_nparray(\u001b[43mDatasetType\u001b[49m\u001b[38;5;241m.\u001b[39mTest)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DatasetType' is not defined"
     ]
    }
   ],
   "source": [
    "# test preds\n",
    "_, test_pred_values = get_preds_as_nparray(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
