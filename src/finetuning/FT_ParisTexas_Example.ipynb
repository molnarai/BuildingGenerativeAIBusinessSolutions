{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41b325a",
   "metadata": {},
   "source": [
    "# Finetuning on a New Dataset - Paris, Texas (finetune on new dataset) Versus Paris, France (pre-trained model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e792a",
   "metadata": {},
   "source": [
    "### Data Handling and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7306e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e8ca0",
   "metadata": {},
   "source": [
    "### LLM model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, TextStreamer, DataCollatorForLanguageModeling\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Saving model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb9afe",
   "metadata": {},
   "source": [
    "### Study response of pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af12e17",
   "metadata": {},
   "source": [
    "#### Loading the model\n",
    "\n",
    "We are going to use Llama 3.2 with only 1 billion parameters.\n",
    "\n",
    "(You can use the 3, 11 or 90 billion version as well.)\n",
    "\n",
    "- Max Sequence Length:\n",
    "    We used max_seq_length 5020.\n",
    "\n",
    "- Loading Llama 3.2 Model:\n",
    "\n",
    "    - The model and tokenizer are loaded using `FastLanguageModel.from_pretrained` with a specific pre-trained model, \"unsloth/Llama-3.2-1B-bnb-4bitt\". \n",
    "    - This is optimized for 4-bit precision, which reduces memory usage and increases training speed without significantly compromising performance.  \n",
    "    - load_in_4bit=True \n",
    "\n",
    "- Applying PEFT (Parameter-Efficient Fine-Tuning):\n",
    "\n",
    "    - Then we configured model using get_peft_model, which applies LoRA (Low-Rank Adaptation) techniques. \n",
    "    - This approach focuses on fine-tuning only specific layers or parts of the model, rather than the entire network.\n",
    "    - This drastically reduces the computational resources needed.\n",
    "\n",
    "- Parameters:\n",
    "\n",
    "    - r=16\n",
    "    - lora_alpha=16 for target_modules (include key components involved in attention mechanisms like q_proj, k_proj, and v_proj)\n",
    "    - use_rslora=True (activates Rank-Stabilized LoRA())\n",
    "    - use_gradient_checkpointing=\"unsloth\" (memory usage optimized during training)\n",
    "\n",
    "- Verifying Trainable Parameters:\n",
    "    We used `model.print_trainable_parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a24bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 5020\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state = 32,\n",
    "    loftq_config = None,\n",
    ")\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa945edd",
   "metadata": {},
   "source": [
    "#### Executing an example prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d6e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prompt = \"\"\"Analyze the text based on what are the top 5 tourist attractions in Paris. Sort results based on popularity.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    data_prompt.format(\n",
    "        #instructions\n",
    "        text,\n",
    "        #answer\n",
    "        \"\",\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 5020, use_cache = True)\n",
    "answer=tokenizer.batch_decode(outputs)\n",
    "answer = answer[0].split(\"### Response:\")[-1]\n",
    "print(\"Answer of the question is:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547922b6",
   "metadata": {},
   "source": [
    "### Calling the dataset for finetuning\n",
    "\n",
    "NOTE: REPLACE DATASET BELOW WITH DATASET ON PARIS, TEXAS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a302d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, pathlib, pymupdf\n",
    "# fname = sys.argv[1]  # get document filename\n",
    "# with pymupdf.open(fname) as doc:  # open document\n",
    "#     text = chr(12).join([page.get_text() for page in doc])\n",
    "# # write as a binary file to support non-ASCII characters\n",
    "# pathlib.Path(fname + \".txt\").write_bytes(text.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c472a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "def get_filenames_with_glob(directory):\n",
    "    \"\"\"\n",
    "    Gets all filenames in a directory using the glob module.\n",
    "\n",
    "    Args:\n",
    "        directory: The path to the directory.\n",
    "\n",
    "    Returns:\n",
    "        A list of filenames in the directory.\n",
    "    \"\"\"\n",
    "    # Ensure the directory path ends with a separator\n",
    "    if not directory.endswith(os.path.sep):\n",
    "        directory += os.path.sep\n",
    "\n",
    "    # Use glob to match all files in the directory\n",
    "    all_files = glob.glob(directory + '*')\n",
    "    \n",
    "    # Filter out directories, keeping only files\n",
    "    filenames = [f for f in all_files if os.path.isfile(f)]\n",
    "\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a18a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib, pymupdf\n",
    "\n",
    "absolute_path = \"/myapp/local/text_files\"\n",
    "directory_path = absolute_path  # Replace with the actual path\n",
    "filenames = get_filenames_with_glob(directory_path)\n",
    "# print(filenames)\n",
    "\n",
    "all_text = \"\"\n",
    "print(\"Filenames in directory:\")\n",
    "for fname in filenames:\n",
    "    print(fname)\n",
    "\n",
    "    with pymupdf.open(fname) as doc:  # open document\n",
    "        text = chr(12).join([page.get_text() for page in doc])\n",
    "        all_text = \" \".join([all_text, text])\n",
    "# print(all_text)\n",
    "    \n",
    "# write as a binary file to support non-ASCII characters\n",
    "file_name = \"/myapp/local/paris_texas_sites\"\n",
    "pathlib.Path(file_name + \".txt\").write_bytes(all_text.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c848cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "file_path = \"/myapp/local/paris_texas_sites.txt\"\n",
    "train_dataset = load_dataset(\"text\", data_files={\"train\": [file_path]}, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.shape)\n",
    "print(train_dataset['text'][:10])\n",
    "print(train_dataset['text'][-10:])\n",
    "# type(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4776030d",
   "metadata": {},
   "source": [
    "### Prepare dataset for finetuning in form of question-answer pairs\n",
    "\n",
    "**Instructions:** \n",
    "\n",
    "Fill in missing code base to convert generate text file into a pandas dataframe from which  question-answer pairs can be extracted:\n",
    "\n",
    "- Column **'Context'** (e.g. \"What is a tourist attarction in Paris? How large was the number of vistors to this site last year?\")\n",
    "- Column **'Response'** (e.g. \"The Eiffel tower is a main attraction in Paris. About 1.5 million vistors visted it last year 2024.\")\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfd25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912c8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd57ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85feb52d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "406874b9",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e07ea7",
   "metadata": {},
   "source": [
    "#### Loading the model\n",
    "\n",
    "We are going to use Llama 3.2 with only 1 billion parameters.\n",
    "\n",
    "(You can use the 3, 11 or 90 billion version as well.)\n",
    "\n",
    "- Max Sequence Length:\n",
    "    We used max_seq_length 5020.\n",
    "\n",
    "- Loading Llama 3.2 Model:\n",
    "\n",
    "    - The model and tokenizer are loaded using `FastLanguageModel.from_pretrained` with a specific pre-trained model, \"unsloth/Llama-3.2-1B-bnb-4bitt\". \n",
    "    - This is optimized for 4-bit precision, which reduces memory usage and increases training speed without significantly compromising performance.  \n",
    "    - load_in_4bit=True \n",
    "\n",
    "- Applying PEFT (Parameter-Efficient Fine-Tuning):\n",
    "\n",
    "    - Then we configured model using get_peft_model, which applies LoRA (Low-Rank Adaptation) techniques. \n",
    "    - This approach focuses on fine-tuning only specific layers or parts of the model, rather than the entire network.\n",
    "    - This drastically reduces the computational resources needed.\n",
    "\n",
    "- Parameters:\n",
    "\n",
    "    - r=16\n",
    "    - lora_alpha=16 for target_modules (include key components involved in attention mechanisms like q_proj, k_proj, and v_proj)\n",
    "    - use_rslora=True (activates Rank-Stabilized LoRA())\n",
    "    - use_gradient_checkpointing=\"unsloth\" (memory usage optimized during training)\n",
    "\n",
    "- Verifying Trainable Parameters:\n",
    "    We used `model.print_trainable_parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec7255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_length = 5020\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
    "#     max_seq_length=max_seq_length,\n",
    "#     load_in_4bit=True,\n",
    "#     dtype=None,\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "\n",
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r=16,\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0,\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
    "#     use_rslora=True,\n",
    "#     use_gradient_checkpointing=\"unsloth\",\n",
    "#     random_state = 32,\n",
    "#     loftq_config = None,\n",
    "# )\n",
    "# print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540cfa7",
   "metadata": {},
   "source": [
    "#### Prepare data for model feed\n",
    "\n",
    "Main points to remember:\n",
    "\n",
    "- Data Prompt Structure:\n",
    "The data_prompt is a formatted string template designed to guide the model in analyzing the provided text. It includes placeholders for the input text (the context) and the model's response. This template specifically prompts the model to identify mental health indicators, making it easier to fine-tune the model for mental health-related tasks.\n",
    "\n",
    "- End-of-Sequence Token:\n",
    "The EOS_TOKEN is retrieved from the tokenizer to signify the end of each text sequence. This token is essential for the model to recognize when a prompt has ended, helping to maintain the structure of the data during training or inference.\n",
    "\n",
    "- Formatting Function:\n",
    "The formatting_prompt used to take a batch of examples and formats them according to the data_prompt. It iterates over the input and output pairs, inserting them into the template and appending the EOS token at the end. The function then returns a dictionary containing the formatted text, ready for model training or evaluation.\n",
    "\n",
    "- Function Output:\n",
    "The function outputs a dictionary where the key is \"text\" and the value is a list of formatted strings. Each string represents a fully prepared prompt for the model, combining the context, response and the structured prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a108180",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prompt = \"\"\"Analyze the text based on what are the top 5 tourist attractions in Paris. Sort results based on popularity.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompt(examples):\n",
    "    inputs       = examples[\"Context\"]\n",
    "    outputs      = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input_, output in zip(inputs, outputs):\n",
    "        text = data_prompt.format(input_, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85700e2e",
   "metadata": {},
   "source": [
    "#### Format the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = Dataset.from_pandas(filtered_data)\n",
    "training_data = training_data.map(formatting_prompt, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4a099",
   "metadata": {},
   "source": [
    "#### Training setup to start fine tuning\n",
    "\n",
    "- Trainer Initialization:\n",
    "We are going to initialize SFTTrainer with the model and tokenizer, as well as the training dataset. \n",
    "\n",
    "- Training Arguments:\n",
    "The TrainingArguments class is used to define key hyperparameters for the training process:\n",
    "\n",
    "    - learning_rate=3e-4: Sets the learning rate for the optimizer.\n",
    "    - per_device_train_batch_size=32: Defines the batch size per device, optimizing GPU usage.\n",
    "    - num_train_epochs=20: Specifies the number of training epochs.\n",
    "    - fp16=not is_bfloat16_supported() and bf16=is_bfloat16_supported(): Enable mixed precision training to reduce memory usage, depending on hardware support.\n",
    "    - optim=\"adamw_8bit\": Uses the 8-bit AdamW optimizer for efficient memory usage.\n",
    "    - weight_decay=0.01: Applies weight decay to prevent overfitting.\n",
    "    - output_dir=\"output\": Specifies the directory where the trained model and logs will be saved.\n",
    "\n",
    "- Training Process:\n",
    "\n",
    "    - Finally we called trainer.train() method to start the training process. \n",
    "    - It uses the defined parameters of our fine-tune the model, adjusting weights and learning from the provided dataset. \n",
    "    - The trainer also handles data packing and gradient accumulation, optimizing the training pipeline for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96df91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=training_data,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=True,\n",
    "    args=TrainingArguments(\n",
    "        learning_rate=3e-4,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_train_epochs=40,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=20,\n",
    "        output_dir=\"output\",\n",
    "        seed=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b44a9",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bcd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What are the top 5 attractions of Paris? Sort by popularity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d176c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFinetuned = FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    data_prompt.format(\n",
    "        #instructions\n",
    "        text,\n",
    "        #answer\n",
    "        \"\",\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = modelFinetuned.generate(**inputs, max_new_tokens = 5020, use_cache = True)\n",
    "answer = tokenizer.batch_decode(outputs)\n",
    "answer = answer[0].split(\"### Response:\")[-1]\n",
    "print(\"Answer of the question is:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc58a69",
   "metadata": {},
   "source": [
    "### Expected response:\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773a6e7",
   "metadata": {},
   "source": [
    "### Push a fine-tuned model and its tokenizer to the Hugging Face Hub\n",
    "\n",
    "**Note:**\n",
    "- Create a **.env** file in your local/ folder in your working directory in the Docker environment (/myapp/local).\n",
    "- Copy the line your **HF_TOKEN=\\<your Hugginface API token\\>** with your Hugginface API token inserted as value into your .env file.\n",
    "- Run the cell below to load **HF_TOKEN** as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e92a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"HF_TOKEN\"] = \"hugging face token key, you can create from your HF account.\"\n",
    "# model.push_to_hub(\"ImranzamanML/1B_finetuned_llama3.2\", use_auth_token=os.getenv(\"HF_TOKEN\"))\n",
    "# tokenizer.push_to_hub(\"ImranzamanML/1B_finetuned_llama3.2\", use_auth_token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d063831",
   "metadata": {},
   "source": [
    "### Save fine-tuned model and its tokenizer locally on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e83887",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model/Paris_Texas_1B_finetuned_llama3.2\")\n",
    "tokenizer.save_pretrained(\"model/Paris_Texas_1B_finetuned_llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e40e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
