{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup Ray Federated Learning Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "import torch\n",
    "from ray import train\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train.torch import TorchConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, PeftModel, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "num_gpus = torch.cuda.device_count()\n",
    "ray.init(\n",
    "    object_store_memory=2 * 1024 * 1024 * 1024,\n",
    "    num_cpus=20,\n",
    "    num_gpus=num_gpus\n",
    ")\n",
    "# Configuration for federated learning\n",
    "fed_config = {\n",
    "    \"base_model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"num_epochs\": 0.5,\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Local Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_local_documents(config):\n",
    "    # Get worker-specific device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Set up local model with LoRA\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config[\"base_model\"],\n",
    "        device_map=\"auto\",\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA adapter\n",
    "    lora_config = LoraConfig(\n",
    "        r=config[\"lora_r\"],\n",
    "        lora_alpha=config[\"lora_alpha\"],\n",
    "        target_modules=config[\"target_modules\"],\n",
    "        lora_dropout=config[\"lora_dropout\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to model\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Set up tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config[\"base_model\"])\n",
    "    \n",
    "    # Get worker ID to identify which partition to use\n",
    "    worker_id = train.get_context().get_worker_id()\n",
    "    \n",
    "    # Process local documents (adapted from existing pipeline)\n",
    "    local_data = process_local_documents(f\"Documents_partition_{worker_id}\")\n",
    "    \n",
    "    # Convert to dataset\n",
    "    dataset = convert_to_dataset(local_data)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./worker_{worker_id}_output\",\n",
    "        per_device_train_batch_size=config[\"batch_size\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        num_train_epochs=config[\"num_epochs\"],\n",
    "        gradient_accumulation_steps=8,\n",
    "        optim=\"adamw_torch\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=training_args,\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Extract LoRA adapter weights only (much smaller than full model)\n",
    "    adapter_weights = extract_lora_weights(model)\n",
    "    \n",
    "    # Report weights back to aggregator\n",
    "    train.report({\"adapter_weights\": adapter_weights})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_documents(source_dir, num_partitions):\n",
    "    \"\"\"Partition documents into separate directories for federated learning\"\"\"\n",
    "    \n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            all_files.append(file_path)\n",
    "    \n",
    "    # Create partition directories\n",
    "    for i in range(num_partitions):\n",
    "        os.makedirs(f\"Documents_partition_{i}\", exist_ok=True)\n",
    "    \n",
    "    # Distribute files across partitions\n",
    "    for i, file_path in enumerate(all_files):\n",
    "        partition_idx = i % num_partitions\n",
    "        dest_path = os.path.join(f\"Documents_partition_{partition_idx}\", \n",
    "                                os.path.basename(file_path))\n",
    "        shutil.copy(file_path, dest_path)\n",
    "    \n",
    "    print(f\"Partitioned {len(all_files)} documents into {num_partitions} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create Aggregation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedavg_aggregate(weights_list):\n",
    "    \"\"\"Implement Federated Averaging (FedAvg) algorithm\"\"\"\n",
    "    \n",
    "    # Initialize aggregated weights with the first model's weights\n",
    "    aggregated_weights = {k: torch.zeros_like(v) for k, v in weights_list[0].items()}\n",
    "    \n",
    "    # Sum all weights\n",
    "    for weights in weights_list:\n",
    "        for key in aggregated_weights:\n",
    "            aggregated_weights[key] += weights[key]\n",
    "    \n",
    "    # Average the weights\n",
    "    num_models = len(weights_list)\n",
    "    for key in aggregated_weights:\n",
    "        aggregated_weights[key] /= num_models\n",
    "    \n",
    "    return aggregated_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_aggregated_weights(base_model, lora_config, aggregated_weights):\n",
    "    \"\"\"Apply aggregated weights to a fresh model\"\"\"\n",
    "    \n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    # Load the aggregated weights\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in aggregated_weights:\n",
    "                param.copy_(aggregated_weights[name])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Orchestrate the Federated Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_federated_training():\n",
    "    # 1. Partition the data\n",
    "    partition_documents(\"Documents_semi_structured\", num_gpus)\n",
    "    \n",
    "    # 2. Setup the trainer\n",
    "    trainer = TorchTrainer(\n",
    "        train_on_local_documents,\n",
    "        train_loop_config=fed_config,\n",
    "        scaling_config=train.ScalingConfig(\n",
    "            num_workers=num_gpus,\n",
    "            use_gpu=True,\n",
    "        ),\n",
    "        torch_config=TorchConfig(backend=\"nccl\")\n",
    "    )\n",
    "    \n",
    "    # 3. Run federated training\n",
    "    results = trainer.fit()\n",
    "    \n",
    "    # 4. Extract all worker model weights\n",
    "    worker_weights = [result[\"adapter_weights\"] for result in results.metrics_dataframe.to_dict('records')]\n",
    "    \n",
    "    # 5. Aggregate weights using FedAvg\n",
    "    aggregated_weights = fedavg_aggregate(worker_weights)\n",
    "    \n",
    "    # 6. Create and save the final model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        fed_config[\"base_model\"],\n",
    "        device_map=\"auto\",\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=fed_config[\"lora_r\"],\n",
    "        lora_alpha=fed_config[\"lora_alpha\"],\n",
    "        target_modules=fed_config[\"target_modules\"],\n",
    "        lora_dropout=fed_config[\"lora_dropout\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    final_model = apply_aggregated_weights(base_model, lora_config, aggregated_weights)\n",
    "    \n",
    "    # 7. Save the final model\n",
    "    save_path = \"./federated_model\"\n",
    "    final_model.save_pretrained(save_path)\n",
    "    \n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Federated Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_evaluation(model_path):\n",
    "    \"\"\"Run distributed evaluation on the federated model\"\"\"\n",
    "    \n",
    "    @ray.remote(num_gpus=1)\n",
    "    def evaluate_on_partition(partition_id, model_path):\n",
    "        # Load model\n",
    "        model = PeftModel.from_pretrained(\n",
    "            AutoModelForCausalLM.from_pretrained(\n",
    "                fed_config[\"base_model\"],\n",
    "                device_map=\"auto\",\n",
    "                load_in_4bit=True\n",
    "            ),\n",
    "            model_path\n",
    "        )\n",
    "        \n",
    "        # Load eval datasets for this partition\n",
    "        eval_datasets = load_evaluation_datasets(f\"eval_partition_{partition_id}\")\n",
    "        \n",
    "        # Run evaluation\n",
    "        metrics = {}\n",
    "        for dataset_name, dataset in eval_datasets.items():\n",
    "            # Implement evaluation logic similar to existing code\n",
    "            dataset_metrics = evaluate_dataset(model, dataset)\n",
    "            metrics[dataset_name] = dataset_metrics\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    # Distribute evaluation across GPUs\n",
    "    futures = [evaluate_on_partition.remote(i, model_path) for i in range(num_gpus)]\n",
    "    results = ray.get(futures)\n",
    "    \n",
    "    # Combine results\n",
    "    combined_metrics = {}\n",
    "    for result in results:\n",
    "        for dataset_name, metrics in result.items():\n",
    "            if dataset_name not in combined_metrics:\n",
    "                combined_metrics[dataset_name] = []\n",
    "            combined_metrics[dataset_name].append(metrics)\n",
    "    \n",
    "    # Average metrics across partitions\n",
    "    final_metrics = {}\n",
    "    for dataset_name, metrics_list in combined_metrics.items():\n",
    "        final_metrics[dataset_name] = {\n",
    "            metric: sum(m[metric] for m in metrics_list) / len(metrics_list)\n",
    "            for metric in metrics_list[0]\n",
    "        }\n",
    "    \n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agenticai_frameworks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
