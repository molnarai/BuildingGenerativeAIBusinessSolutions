{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Graph RAG\n",
    "\n",
    "Source: \n",
    "Stephen Collins, **Example Graph RAG**, https://github.com/stephenc222/example-graphrag/tree/main\n",
    "\n",
    "[paper](https://arxiv.org/abs/2404.16130) \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\" by Darren Edge et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'bayanpy', 'wurlitzer', 'graph_tool', 'infomap', 'leidenalg'}\n",
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'ASLPAw', 'pyclustering'}\n",
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'wurlitzer', 'infomap', 'leidenalg'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import networkx as nx\n",
    "from cdlib import algorithms\n",
    "from dotenv import load_dotenv\n",
    "from document_reader import read_documents_from_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# openai_api_key = open(\"/Users/mjack6/.secrets/openai_mjack.apikey\", \"r\").read().strip()\n",
    "# os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "openai_api_key = open(\"/home/mjack6/.secrets/openai_mjack.apikey\", \"r\").read().strip()\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Source Documents → Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Source Documents → Text Chunks\n",
    "def split_documents_into_chunks(documents, chunk_size=600, overlap_size=100):\n",
    "    chunks = []\n",
    "    for document in documents:\n",
    "        for i in range(0, len(document), chunk_size - overlap_size):\n",
    "            chunk = document[i:i + chunk_size]\n",
    "            chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Chunks → Element Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Text Chunks → Element Instances\n",
    "def extract_elements_from_chunks(chunks):\n",
    "    elements = []\n",
    "    for index, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk index {index} of {len(chunks)}:\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Extract entities and relationships from the following text.\"},\n",
    "                {\"role\": \"user\", \"content\": chunk}\n",
    "            ]\n",
    "        )\n",
    "        print(response.choices[0].message.content)\n",
    "        entities_and_relations = response.choices[0].message.content\n",
    "        elements.append(entities_and_relations)\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Element Instances → Element Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Element Instances → Element Summaries\n",
    "def summarize_elements(elements):\n",
    "    summaries = []\n",
    "    for index, element in enumerate(elements):\n",
    "        print(f\"Element index {index} of {len(elements)}:\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Summarize the following entities and relationships in a structured format. Use \\\"->\\\" to represent relationships, after the \\\"Relationships:\\\" word.\"},\n",
    "                {\"role\": \"user\", \"content\": element}\n",
    "            ]\n",
    "        )\n",
    "        print(\"Element summary:\", response.choices[0].message.content)\n",
    "        summary = response.choices[0].message.content\n",
    "        summaries.append(summary)\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Element Summaries → Graph Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Element Summaries → Graph Communities\n",
    "def build_graph_from_summaries(summaries):\n",
    "    G = nx.Graph()\n",
    "    for index, summary in enumerate(summaries):\n",
    "        print(f\"Summary index {index} of {len(summaries)}:\")\n",
    "        lines = summary.split(\"\\n\")\n",
    "        entities_section = False\n",
    "        relationships_section = False\n",
    "        entities = []\n",
    "        for line in lines:\n",
    "            if line.startswith(\"### Entities:\") or line.startswith(\"**Entities:**\"):\n",
    "                entities_section = True\n",
    "                relationships_section = False\n",
    "                continue\n",
    "            elif line.startswith(\"### Relationships:\") or line.startswith(\"**Relationships:**\"):\n",
    "                entities_section = False\n",
    "                relationships_section = True\n",
    "                continue\n",
    "            if entities_section and line.strip():\n",
    "                if line[0].isdigit() and line[1] == \".\":\n",
    "                    line = line.split(\".\", 1)[1].strip()\n",
    "                entity = line.strip()\n",
    "                entity = entity.replace(\"**\", \"\")\n",
    "                entities.append(entity)\n",
    "                G.add_node(entity)\n",
    "            elif relationships_section and line.strip():\n",
    "                parts = line.split(\"->\")\n",
    "                if len(parts) >= 2:\n",
    "                    source = parts[0].strip()\n",
    "                    target = parts[-1].strip()\n",
    "                    relation = \" -> \".join(parts[1:-1]).strip()\n",
    "                    G.add_edge(source, target, label=relation)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Graph Communities → Community Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Graph Communities → Community Summaries\n",
    "def detect_communities(graph):\n",
    "    communities = []\n",
    "    index = 0\n",
    "    for component in nx.connected_components(graph):\n",
    "        print(\n",
    "            f\"Component index {index} of {len(list(nx.connected_components(graph)))}:\")\n",
    "        subgraph = graph.subgraph(component)\n",
    "        if len(subgraph.nodes) > 1:  # Leiden algorithm requires at least 2 nodes\n",
    "            try:\n",
    "                sub_communities = algorithms.leiden(subgraph)\n",
    "                for community in sub_communities.communities:\n",
    "                    communities.append(list(community))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing community {index}: {e}\")\n",
    "        else:\n",
    "            communities.append(list(subgraph.nodes))\n",
    "        index += 1\n",
    "    print(\"Communities from detect_communities:\", communities)\n",
    "    return communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_communities(communities, graph):\n",
    "    community_summaries = []\n",
    "    for index, community in enumerate(communities):\n",
    "        print(f\"Summarize Community index {index} of {len(communities)}:\")\n",
    "        subgraph = graph.subgraph(community)\n",
    "        nodes = list(subgraph.nodes)\n",
    "        edges = list(subgraph.edges(data=True))\n",
    "        description = \"Entities: \" + \", \".join(nodes) + \"\\nRelationships: \"\n",
    "        relationships = []\n",
    "        for edge in edges:\n",
    "            relationships.append(\n",
    "                f\"{edge[0]} -> {edge[2]['label']} -> {edge[1]}\")\n",
    "        description += \", \".join(relationships)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Summarize the following community of entities and relationships.\"},\n",
    "                {\"role\": \"user\", \"content\": description}\n",
    "            ]\n",
    "        )\n",
    "        summary = response.choices[0].message.content.strip()\n",
    "        community_summaries.append(summary)\n",
    "    return community_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Community Summaries → Community Answers → Global Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Community Summaries → Community Answers → Global Answer\n",
    "def generate_answers_from_communities(community_summaries, query):\n",
    "    intermediate_answers = []\n",
    "    for index, summary in enumerate(community_summaries):\n",
    "        print(f\"Summary index {index} of {len(community_summaries)}:\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Answer the following query based on the provided summary.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Query: {query} Summary: {summary}\"}\n",
    "            ]\n",
    "        )\n",
    "        print(\"Intermediate answer:\", response.choices[0].message.content)\n",
    "        intermediate_answers.append(\n",
    "            response.choices[0].message.content)\n",
    "\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\",\n",
    "                \"content\": \"Combine these answers into a final, concise response.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Intermediate answers: {intermediate_answers}\"}\n",
    "        ]\n",
    "    )\n",
    "    final_answer = final_response.choices[0].message.content\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting It All Together\n",
    "def graph_rag_pipeline(documents, query, chunk_size=600, overlap_size=100):\n",
    "    # Step 1: Split documents into chunks\n",
    "    chunks = split_documents_into_chunks(\n",
    "        documents, chunk_size, overlap_size)\n",
    "\n",
    "    # Step 2: Extract elements from chunks\n",
    "    elements = extract_elements_from_chunks(chunks)\n",
    "\n",
    "    # Step 3: Summarize elements\n",
    "    summaries = summarize_elements(elements)\n",
    "\n",
    "    # Step 4: Build graph and detect communities\n",
    "    graph = build_graph_from_summaries(summaries)\n",
    "    print(\"graph:\", graph)\n",
    "    communities = detect_communities(graph)\n",
    "\n",
    "    print(\"communities:\", communities[0])\n",
    "    # Step 5: Summarize communities\n",
    "    community_summaries = summarize_communities(communities, graph)\n",
    "\n",
    "    # Step 6: Generate answers from community summaries\n",
    "    final_answer = generate_answers_from_communities(\n",
    "        community_summaries, query)\n",
    "\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read documents and store them in the DOCUMENTS list\n",
    "DOCUMENTS = read_documents_from_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = \"What are the main themes in these documents?\"\n",
    "print('Query:', query)\n",
    "answer = graph_rag_pipeline(DOCUMENTS, query)\n",
    "print('Answer:', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSA8700 Python 3.12",
   "language": "python",
   "name": "conda-msa8700"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
