# Build stage for llama.cpp
FROM ubuntu:22.04 as builder

# Install build dependencies
RUN apt-get update && apt-get install -y \
    git \
    cmake \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Clone and build llama.cpp
WORKDIR /build
RUN git clone https://github.com/ggml-org/llama.cpp && \
    cd llama.cpp && \
    mkdir build && \
    cd build && \
    cmake .. && \
    cmake --build . --config Release

# Final stage using Hugging Face container as base
FROM huggingface/transformers-pytorch-gpu
RUN mkdir -p /myapp/llama.cpp
WORKDIR /myapp
RUN cd /myapp

# COPY --from=builder /build/llama.cpp/build/bin/llama-quantize /usr/local/bin/
COPY --from=builder /build/llama.cpp/build/* llama.cpp/

# Install cmake
# RUN apt-get update && apt-get install -y cmake
COPY ./requirements.txt .
RUN pip3 install  -r ./requirements.txt --root-user-action ignore
COPY src .
# ENTRYPOINT [ "/bin/bash" ]
# CMD [ "pwd", ";", "ls -lR" ]
