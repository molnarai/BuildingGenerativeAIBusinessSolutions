{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RoBERTa with Fastai - CB Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "\n",
    "Dev Sharma, Using RoBERTa with fast.ai for SuperGLUE Task CB\n",
    "Finetuning state of the art RoBERTa with fast.ai on Commitment Bank NLP task.\n",
    "\n",
    "This notebook follows the tutorial @ https://medium.com/@devkosal/superglue-roberta-with-fastai-for-rte-task-c362961be957\n",
    "\n",
    "Related article:\n",
    "\n",
    "Using RoBERTa with fast.ai for NLP\n",
    "Implementing the current state of the art NLP model in fast.ai\n",
    "\n",
    "https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastai\n",
      "  Downloading fastai-2.7.18-py3-none-any.whl (234 kB)\n",
      "\u001b[K     |████████████████████████████████| 234 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=9.0.0 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from fastai) (11.1.0)\n",
      "Requirement already satisfied: pyyaml in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from fastai) (6.0.2)\n",
      "Collecting fastdownload<2,>=0.0.5\n",
      "  Downloading fastdownload-0.0.7-py3-none-any.whl (12 kB)\n",
      "Collecting torch<2.6,>=1.10\n",
      "  Downloading torch-2.5.1-cp310-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 63.9 MB 20.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from fastai) (1.6.1)\n",
      "Collecting fastcore<1.8,>=1.5.29\n",
      "  Downloading fastcore-1.7.29-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 14.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from fastai) (24.2)\n",
      "Requirement already satisfied: pip in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from fastai) (21.2.3)\n",
      "Collecting spacy<4\n",
      "  Downloading spacy-3.8.4-cp310-cp310-macosx_11_0_arm64.whl (6.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 227 kB/s eta 0:00:0101\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from fastai) (2.32.3)\n",
      "Requirement already satisfied: matplotlib in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from fastai) (3.10.0)\n",
      "Requirement already satisfied: pandas in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from fastai) (2.2.3)\n",
      "Requirement already satisfied: torchvision>=0.11 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from fastai) (0.21.0)\n",
      "Collecting fastprogress>=0.2.4\n",
      "  Downloading fastprogress-1.0.3-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scipy in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from fastai) (1.15.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from spacy<4->fastai) (0.15.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from spacy<4->fastai) (3.1.5)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.12-cp310-cp310-macosx_11_0_arm64.whl (26 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from spacy<4->fastai) (4.67.1)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp310-cp310-macosx_11_0_arm64.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 27.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<8.4.0,>=8.3.4\n",
      "  Downloading thinc-8.3.4-cp310-cp310-macosx_11_0_arm64.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 46.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting weasel<0.5.0,>=0.1.0\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 43.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from spacy<4->fastai) (2.10.6)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.11-cp310-cp310-macosx_11_0_arm64.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 4.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.5.1-cp310-cp310-macosx_11_0_arm64.whl (634 kB)\n",
      "\u001b[K     |████████████████████████████████| 634 kB 40.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from spacy<4->fastai) (1.26.4)\n",
      "Requirement already satisfied: setuptools in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from spacy<4->fastai) (57.4.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 36.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting language-data>=1.2\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 152.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting marisa-trie>=1.1.0\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-macosx_11_0_arm64.whl (174 kB)\n",
      "\u001b[K     |████████████████████████████████| 174 kB 26.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.12.2 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (2.27.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from requests->fastai) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from requests->fastai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from requests->fastai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from requests->fastai) (2.3.0)\n",
      "Collecting blis<1.3.0,>=1.2.0\n",
      "  Downloading blis-1.2.0-cp310-cp310-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 25.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from torch<2.6,>=1.10->fastai) (1.13.1)\n",
      "Requirement already satisfied: filelock in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from torch<2.6,>=1.10->fastai) (3.17.0)\n",
      "Requirement already satisfied: fsspec in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from torch<2.6,>=1.10->fastai) (2024.9.0)\n",
      "Requirement already satisfied: networkx in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from torch<2.6,>=1.10->fastai) (3.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from sympy==1.13.1->torch<2.6,>=1.10->fastai) (1.3.0)\n",
      "Collecting torchvision>=0.11\n",
      "  Downloading torchvision-0.20.1-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 74.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: rich>=10.11.0 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (13.9.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (0.1.2)\n",
      "Collecting smart-open<8.0.0,>=5.2.1\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 3.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4->fastai) (1.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from jinja2->spacy<4->fastai) (3.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from matplotlib->fastai) (3.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from matplotlib->fastai) (1.4.8)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from matplotlib->fastai) (4.55.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from matplotlib->fastai) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from matplotlib->fastai) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from matplotlib->fastai) (0.12.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->fastai) (1.17.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from pandas->fastai) (2025.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from pandas->fastai) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from scikit-learn->fastai) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/lib/python3.10/site-packages (from scikit-learn->fastai) (3.5.0)\n",
      "Installing collected packages: catalogue, srsly, murmurhash, marisa-trie, cymem, wasabi, smart-open, preshed, language-data, confection, cloudpathlib, blis, weasel, torch, thinc, spacy-loggers, spacy-legacy, langcodes, fastprogress, fastcore, torchvision, spacy, fastdownload, fastai\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0\n",
      "    Uninstalling torch-2.6.0:\n",
      "      Successfully uninstalled torch-2.6.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.21.0\n",
      "    Uninstalling torchvision-0.21.0:\n",
      "      Successfully uninstalled torchvision-0.21.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xformers 0.0.29.post2 requires torch>=2.6, but you have torch 2.5.1 which is incompatible.\u001b[0m\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 fastai-2.7.18 fastcore-1.7.29 fastdownload-0.0.7 fastprogress-1.0.3 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 torch-2.5.1 torchvision-0.20.1 wasabi-1.1.3 weasel-0.4.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Users/mjack6/.pyenv/versions/3.10.0/envs/venv_finetuning/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T02:30:58.738741Z",
     "start_time": "2019-09-04T02:30:57.798726Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from fastai.metrics import *\n",
    "from transformers import RobertaTokenizer\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T02:30:58.744883Z",
     "start_time": "2019-09-04T02:30:58.740327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a config object to store task specific information\n",
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "        \n",
    "config = Config(\n",
    "    task = \"CB\",\n",
    "    testing=False,\n",
    "    seed = 2019,\n",
    "    roberta_model_name='roberta-base', # can also be exchanged with roberta-large \n",
    "    max_lr=1e-5,\n",
    "    epochs=10,\n",
    "    use_fp16=False,\n",
    "    bs=4, \n",
    "    max_seq_len=256, \n",
    "    num_labels = 3,\n",
    "    hidden_dropout_prob=.05,\n",
    "    hidden_size=768, # 1024 for roberta-large\n",
    "    start_tok = \"<s>\",\n",
    "    end_tok = \"</s>\",\n",
    "    mark_fields=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T02:30:58.756712Z",
     "start_time": "2019-09-04T02:30:58.746261Z"
    }
   },
   "outputs": [],
   "source": [
    "path = Path(\".\")\n",
    "data_path = path/\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T02:31:00.072931Z",
     "start_time": "2019-09-04T02:31:00.050079Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tj/cfj9qmvs7150dnbbhjnzz1tw0000gq/T/ipykernel_77433/1491146896.py:1: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  train = pd.read_json(data_path/config.task/\"train.jsonl\",lines=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m val \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(data_path\u001b[38;5;241m/\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m,lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(data_path\u001b[38;5;241m/\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m,lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/venv_finetuning/lib/python3.10/site-packages/pandas/io/json/_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/venv_finetuning/lib/python3.10/site-packages/pandas/io/json/_json.py:1023\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         data \u001b[38;5;241m=\u001b[39m ensure_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m   1022\u001b[0m         data_lines \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1023\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_lines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/.pyenv/versions/venv_finetuning/lib/python3.10/site-packages/pandas/io/json/_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m   1049\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/venv_finetuning/lib/python3.10/site-packages/pandas/io/json/_json.py:1187\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/venv_finetuning/lib/python3.10/site-packages/pandas/io/json/_json.py:1403\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1399\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1403\u001b[0m         \u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m     )\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1406\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1407\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1409\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "train = pd.read_json(data_path/config.task/\"train.jsonl\",lines=True)\n",
    "val = pd.read_json(data_path/config.task/\"val.jsonl\",lines=True)\n",
    "test = pd.read_json(data_path/config.task/\"test.jsonl\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.053988Z",
     "start_time": "2019-09-03T23:52:36.049310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 3)\n"
     ]
    }
   ],
   "source": [
    "# drop the unnecessary idx column\n",
    "for df in (train,val):\n",
    "    if \"idx\" in df.columns: df.drop(\"idx\",axis=1,inplace=True)\n",
    "        \n",
    "if config.testing:\n",
    "    train = train[:100]\n",
    "    val = val[:100]\n",
    "    \n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.067099Z",
     "start_time": "2019-09-03T23:52:36.055142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>premise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the language was peeled down</td>\n",
       "      <td>entailment</td>\n",
       "      <td>It was a complex language. Not written down bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no women are allowed to take part in this ritual</td>\n",
       "      <td>entailment</td>\n",
       "      <td>It is part of their religion, a religion I do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gustave was shepherded into creative retreat a...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>The Paris to Rouen railway was being extended ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gustave was driven to creative retreat in Croi...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Part of it was to be compulsorily purchased. Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buying places is a hobby</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Some of them, like for instance the farm in Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          hypothesis       label  \\\n",
       "0                       the language was peeled down  entailment   \n",
       "1   no women are allowed to take part in this ritual  entailment   \n",
       "2  Gustave was shepherded into creative retreat a...  entailment   \n",
       "3  Gustave was driven to creative retreat in Croi...  entailment   \n",
       "4                           buying places is a hobby  entailment   \n",
       "\n",
       "                                             premise  \n",
       "0  It was a complex language. Not written down bu...  \n",
       "1  It is part of their religion, a religion I do ...  \n",
       "2  The Paris to Rouen railway was being extended ...  \n",
       "3  Part of it was to be compulsorily purchased. Y...  \n",
       "4  Some of them, like for instance the farm in Co...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.077736Z",
     "start_time": "2019-09-03T23:52:36.068230Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contradiction    119\n",
       "entailment       115\n",
       "neutral           16\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.086245Z",
     "start_time": "2019-09-03T23:52:36.079426Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_cols = [\"premise\",\"hypothesis\"]\n",
    "label_cols = \"label\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.093982Z",
     "start_time": "2019-09-03T23:52:36.087757Z"
    }
   },
   "outputs": [],
   "source": [
    "class FastAiRobertaTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n",
    "    def __init__(self, tokenizer: RobertaTokenizer, max_seq_len: int=128, **kwargs): \n",
    "        self._pretrained_tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len \n",
    "    def __call__(self, *args, **kwargs): \n",
    "        return self \n",
    "    def tokenizer(self, t:str) -> List[str]: \n",
    "        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n",
    "        if config.mark_fields:\n",
    "            sub = 2 # subtraction in totoal seq_length to be made due to adding spcl tokens\n",
    "            assert \"xxfld\" in t\n",
    "            t = t.replace(\"xxfld 1\",\"\") # remove the xxfld 1 special token from fastai\n",
    "            # converting fastai field sep token to Roberta\n",
    "            t = re.split(r'xxfld \\d+', t) \n",
    "            res = []\n",
    "            for i in range(len(t)-1): # loop over the number of additional fields and the Roberta sep\n",
    "                res += self._pretrained_tokenizer.tokenize(t[i]) + [config.end_tok, config.end_tok]\n",
    "                sub += 2 # increase our subtractions since we added more spcl tokens\n",
    "            res += self._pretrained_tokenizer.tokenize(t[-1]) # add the last sequence\n",
    "            return [config.start_tok] + res[:self.max_seq_len - sub] + [config.end_tok] \n",
    "        \n",
    "        res = self._pretrained_tokenizer.tokenize(t)\n",
    "        return [config.start_tok] + res[:self.max_seq_len - sub] + [config.end_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.427223Z",
     "start_time": "2019-09-03T23:52:36.095149Z"
    }
   },
   "outputs": [],
   "source": [
    "# create fastai tokenizer for roberta\n",
    "roberta_tok = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "fastai_tokenizer = Tokenizer(tok_func=FastAiRobertaTokenizer(roberta_tok, max_seq_len=config.max_seq_len), \n",
    "                             pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.549924Z",
     "start_time": "2019-09-03T23:52:36.428608Z"
    }
   },
   "outputs": [],
   "source": [
    "# create fastai vocabulary for roberta\n",
    "roberta_tok.save_vocabulary(path)\n",
    "\n",
    "with open('vocab.json', 'r') as f:\n",
    "    roberta_vocab_dict = json.load(f)\n",
    "    \n",
    "fastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.555292Z",
     "start_time": "2019-09-03T23:52:36.551292Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting up pre-processors\n",
    "class RobertaTokenizeProcessor(TokenizeProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "         super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False, mark_fields=config.mark_fields)\n",
    "\n",
    "class RobertaNumericalizeProcessor(NumericalizeProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, vocab=fastai_roberta_vocab, **kwargs)\n",
    "\n",
    "\n",
    "def get_roberta_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
    "    \"\"\"\n",
    "    Constructing preprocessors for Roberta\n",
    "    We remove sos and eos tokens since we add that ourselves in the tokenizer.\n",
    "    We also use a custom vocabulary to match the numericalization with the original Roberta model.\n",
    "    \"\"\"\n",
    "    return [RobertaTokenizeProcessor(tokenizer=tokenizer), NumericalizeProcessor(vocab=vocab)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.563667Z",
     "start_time": "2019-09-03T23:52:36.556702Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a Roberta specific DataBunch class\n",
    "class RobertaDataBunch(TextDataBunch):\n",
    "    \"Create a `TextDataBunch` suitable for training Roberta\"\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=64, val_bs:int=None, pad_idx=1,\n",
    "               pad_first=True, device:torch.device=None, no_check:bool=False, backwards:bool=False, \n",
    "               dl_tfms:Optional[Collection[Callable]]=None, **dl_kwargs) -> DataBunch:\n",
    "        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        val_bs = ifnone(val_bs, bs)\n",
    "        collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
    "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs)\n",
    "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
    "        dataloaders = [train_dl]\n",
    "        for ds in datasets[1:]:\n",
    "            lengths = [len(t) for t in ds.x.items]\n",
    "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
    "        return cls(*dataloaders, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:36.570550Z",
     "start_time": "2019-09-03T23:52:36.565011Z"
    }
   },
   "outputs": [],
   "source": [
    "class RobertaTextList(TextList):\n",
    "    _bunch = RobertaDataBunch\n",
    "    _label_cls = TextList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:38.226583Z",
     "start_time": "2019-09-03T23:52:36.571900Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading the tokenizer and vocab processors\n",
    "processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)\n",
    "\n",
    "# creating our databunch \n",
    "data = ItemLists(\".\", RobertaTextList.from_df(train, \".\", cols=feat_cols, processor=processor),\n",
    "                      RobertaTextList.from_df(val, \".\", cols=feat_cols, processor=processor)\n",
    "                ) \\\n",
    "       .label_from_df(cols=label_cols, label_cls=CategoryList) \\\n",
    "       .add_test(RobertaTextList.from_df(test, \".\", cols=feat_cols, processor=processor)) \\\n",
    "       .databunch(bs=config.bs,pad_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:38.232898Z",
     "start_time": "2019-09-03T23:52:38.228130Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "# defining our model architecture \n",
    "class RobertaForSequenceClassificationModel(nn.Module):\n",
    "    def __init__(self,num_labels=config.num_labels):\n",
    "        super(RobertaForSequenceClassificationModel,self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta = RobertaForSequenceClassification.from_pretrained(config.roberta_model_name,num_labels= self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(input_ids, token_type_ids, attention_mask)\n",
    "        logits = outputs[0] \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:52:41.690696Z",
     "start_time": "2019-09-03T23:52:38.234218Z"
    }
   },
   "outputs": [],
   "source": [
    "roberta_model = RobertaForSequenceClassificationModel() \n",
    "\n",
    "learn = Learner(data, roberta_model, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:54:57.085222Z",
     "start_time": "2019-09-03T23:52:41.692566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.079415</td>\n",
       "      <td>1.069071</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.960143</td>\n",
       "      <td>0.842565</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.768365</td>\n",
       "      <td>0.631433</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.477261</td>\n",
       "      <td>0.324045</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.279063</td>\n",
       "      <td>0.182191</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.151150</td>\n",
       "      <td>0.165908</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.079247</td>\n",
       "      <td>0.116142</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.043399</td>\n",
       "      <td>0.111627</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.103498</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.020711</td>\n",
       "      <td>0.102435</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.model.roberta.train() # setting roberta to train as it is in eval mode by default\n",
    "learn.fit_one_cycle(config.epochs, max_lr=config.max_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:54:57.090526Z",
     "start_time": "2019-09-03T23:54:57.086949Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
    "    learn.model.roberta.eval()\n",
    "    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n",
    "    sampler = [i for i in data.dl(ds_type).sampler]\n",
    "    reverse_sampler = np.argsort(sampler)\n",
    "    ordered_preds = preds[reverse_sampler, :]\n",
    "    pred_values = np.argmax(ordered_preds, axis=1)\n",
    "    return ordered_preds, pred_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:54:57.934272Z",
     "start_time": "2019-09-03T23:54:57.091801Z"
    }
   },
   "outputs": [],
   "source": [
    "# val preds\n",
    "preds, pred_values = get_preds_as_nparray(DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:54:57.939243Z",
     "start_time": "2019-09-03T23:54:57.935988Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9642857142857143"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy for valid valid\n",
    "(pred_values == data.valid_ds.y.items).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T23:55:00.674531Z",
     "start_time": "2019-09-03T23:54:57.940330Z"
    }
   },
   "outputs": [],
   "source": [
    "# test preds\n",
    "_, test_pred_values = get_preds_as_nparray(DatasetType.Test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
