{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6faa4043-61e3-40ef-97aa-999f4ae9d194",
   "metadata": {},
   "source": [
    "# Token-based Metrics\n",
    "\n",
    "1. Jaccard Index: Measures the overlap between two sets of unique tokens. It gives a score based on how many words are shared between the two texts relative to the union of their words.\n",
    "2. Dice Coefficient: Focuses on the overlap between two sets but gives more weight to the intersection by multiplying it by 2.\n",
    "3. Cosine Similarity: Converts the text into vectorized representations (using word counts in this example) and calculates the cosine of the angle between the two vectors. It considers the frequency of words, which is useful for capturing subtle differences in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b3aab0-5391-415e-bb57-ad24f192b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "jp = os.path.join\n",
    "import sys\n",
    "import datetime\n",
    "import json\n",
    "T_now = datetime.datetime.now\n",
    "from openai import OpenAI\n",
    "\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Function to calculate Jaccard Index\n",
    "def jaccard_index(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union\n",
    "\n",
    "# Function to calculate Dice Coefficient\n",
    "def dice_coefficient(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    return 2 * intersection / (len(set1) + len(set2))\n",
    "\n",
    "# Function to calculate Cosine Similarity\n",
    "def cosine_similarity_custom(text1, text2):\n",
    "    vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    return cosine_similarity(vectors)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcefe79-d097-4bd9-a1b6-42bca8bb7ce5",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4eaa320-cb8b-461f-812e-061443cefed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_squad_data():\n",
    "    url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"\n",
    "    data = requests.get(url).json()\n",
    "    print(f\"Number of records: {len(data['data']):,}\")\n",
    "    questions_answers = []\n",
    "    for j, article in enumerate(data['data']):\n",
    "        if j>20: break\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qas in paragraph['qas']:\n",
    "                # print(qas)\n",
    "                # if not qas.get('is_impossible'):\n",
    "                question = qas['question']\n",
    "                answer = qas['answers'][0]['text']  # Take the first answer\n",
    "                questions_answers.append((question, answer))\n",
    "    return questions_answers  # Limit to 10 for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee91ba35-d2de-4c61-b310-b11f453cba6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 442\n"
     ]
    }
   ],
   "source": [
    "qa = load_squad_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "167bef28-4f79-492f-adf9-47c47d15088e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       "  'Saint Bernadette Soubirous'),\n",
       " ('What is in front of the Notre Dame Main Building?',\n",
       "  'a copper statue of Christ'),\n",
       " ('The Basilica of the Sacred heart at Notre Dame is beside to which structure?',\n",
       "  'the Main Building'),\n",
       " ('What is the Grotto at Notre Dame?',\n",
       "  'a Marian place of prayer and reflection'),\n",
       " ('What sits on top of the Main Building at Notre Dame?',\n",
       "  'a golden statue of the Virgin Mary'),\n",
       " ('When did the Scholastic Magazine of Notre dame begin publishing?',\n",
       "  'September 1876'),\n",
       " (\"How often is Notre Dame's the Juggler published?\", 'twice'),\n",
       " ('What is the daily student paper at Notre Dame called?', 'The Observer'),\n",
       " ('How many student news papers are found at Notre Dame?', 'three'),\n",
       " ('In what year did the student paper Common Sense begin publication at Notre Dame?',\n",
       "  '1987')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf9012ea-2bb8-431c-98c0-745dfece21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = open(jp(os.path.expanduser(\"~\"), \".secrets\", \"openai_pmolnar_gsu_edu_msa8700.apikey\"), \"r\").read().strip()\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "client = OpenAI(api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27ca7603-3f63-4624-a956-f8b6a7caf543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(openai_client, question):\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        max_tokens=100,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Answer the following question:\\n\\n{question}\\n\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cbab56b-355c-4a8f-a4f5-a1969eced298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Virgin Mary allegedly appeared to a young girl named Bernadette Soubirous in 1858 in Lourdes, France.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_llm(client, \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff8dd3-773b-4e4f-a46f-75baba4388cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60a8b3d0-a2e9-40ef-bd43-819fc33ff6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llm_on_qa(qa_data, openai_client):\n",
    "    # Load QA dataset\n",
    "    # qa_data = load_squad_data()\n",
    "    results = []\n",
    "    \n",
    "    for qa_pair in qa_data:\n",
    "        question, reference_answer = qa_pair\n",
    "        \n",
    "        # Get the LLM's answer\n",
    "        try:\n",
    "            llm_answer = query_llm(openai_client, question)\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying LLM: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Tokenize the answers into sets of words\n",
    "        ref_tokens = set(reference_answer.lower().split())\n",
    "        llm_tokens = set(llm_answer.lower().split())\n",
    "        \n",
    "        # Calculate the metrics\n",
    "        jaccard = jaccard_index(ref_tokens, llm_tokens)\n",
    "        dice = dice_coefficient(ref_tokens, llm_tokens)\n",
    "        cosine_sim = cosine_similarity_custom(reference_answer, llm_answer)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"Question\": question,\n",
    "            \"Reference Answer\": reference_answer,\n",
    "            \"LLM Answer\": llm_answer,\n",
    "            \"Jaccard Index\": jaccard,\n",
    "            \"Dice Coefficient\": dice,\n",
    "            \"Cosine Similarity\": cosine_sim\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e24ea710-ecf4-442b-bdc8-e1f06b4707a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:06.659140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Reference Answer</th>\n",
       "      <th>LLM Answer</th>\n",
       "      <th>Jaccard Index</th>\n",
       "      <th>Dice Coefficient</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>The Virgin Mary allegedly appeared to a young ...</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.272166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>In front of the Notre Dame Main Building, you ...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.191180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>The Basilica of the Sacred Heart at Notre Dame...</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.350070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>The Grotto at Notre Dame, officially known as ...</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.223782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>The Main Building at the University of Notre D...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.635085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                          Reference Answer  \\\n",
       "0               Saint Bernadette Soubirous   \n",
       "1                a copper statue of Christ   \n",
       "2                        the Main Building   \n",
       "3  a Marian place of prayer and reflection   \n",
       "4       a golden statue of the Virgin Mary   \n",
       "\n",
       "                                          LLM Answer  Jaccard Index  \\\n",
       "0  The Virgin Mary allegedly appeared to a young ...       0.117647   \n",
       "1  In front of the Notre Dame Main Building, you ...       0.066667   \n",
       "2  The Basilica of the Sacred Heart at Notre Dame...       0.045455   \n",
       "3  The Grotto at Notre Dame, officially known as ...       0.056604   \n",
       "4  The Main Building at the University of Notre D...       0.250000   \n",
       "\n",
       "   Dice Coefficient  Cosine Similarity  \n",
       "0          0.210526           0.272166  \n",
       "1          0.125000           0.191180  \n",
       "2          0.086957           0.350070  \n",
       "3          0.107143           0.223782  \n",
       "4          0.400000           0.635085  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "T_0 = T_now()\n",
    "stats = evaluate_llm_on_qa(qa[:5], client)\n",
    "evaluation_df = pd.DataFrame.from_records(stats)\n",
    "print(f\"Elapsed time: {T_now()-T_0}\")\n",
    "\n",
    "evaluation_df.to_csv(\"token_based_metrics_example_table.csv\", index=None)\n",
    "display(evaluation_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa359d-a9b7-455d-8e7e-85048361f400",
   "metadata": {},
   "source": [
    "# ROUGE (Recall-Oriented Understudy for Gisting Evaluation).\n",
    "\n",
    "This example computes ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest common subsequence).\n",
    "\n",
    "It calculates precision, recall, and F1-score for given reference and candidate texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6ab438f-91ba-49d7-975d-5dadb90e49ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Function to calculate n-grams\n",
    "def get_ngrams(text, n=1):\n",
    "    \"\"\"Generate n-grams from the given text.\"\"\"\n",
    "    tokens = text.lower().split()\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return Counter(ngrams)\n",
    "\n",
    "# ROUGE-1 and ROUGE-2 implementation\n",
    "def rouge_n(reference, candidate, n=1):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE-N (recall, precision, and F1-score) for n-grams.\n",
    "    \n",
    "    Args:\n",
    "        reference (str): The reference text.\n",
    "        candidate (str): The candidate text generated by the model.\n",
    "        n (int): The size of n-grams (1 for ROUGE-1, 2 for ROUGE-2).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Recall, precision, and F1-score for ROUGE-N.\n",
    "    \"\"\"\n",
    "    ref_ngrams = get_ngrams(reference, n)\n",
    "    cand_ngrams = get_ngrams(candidate, n)\n",
    "    \n",
    "    # Calculate overlap\n",
    "    overlap = sum((ref_ngrams & cand_ngrams).values())\n",
    "    ref_count = sum(ref_ngrams.values())\n",
    "    cand_count = sum(cand_ngrams.values())\n",
    "    \n",
    "    # Calculate recall, precision, and F1-score\n",
    "    recall = overlap / ref_count if ref_count > 0 else 0.0\n",
    "    precision = overlap / cand_count if cand_count > 0 else 0.0\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1_score\": f1_score\n",
    "    }\n",
    "\n",
    "# Longest Common Subsequence (LCS) calculation\n",
    "def lcs(X, Y):\n",
    "    \"\"\"\n",
    "    Compute the length of the Longest Common Subsequence (LCS) between two sequences.\n",
    "    \"\"\"\n",
    "    m, n = len(X), len(Y)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if X[i - 1] == Y[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
    "    return dp[m][n]\n",
    "\n",
    "# ROUGE-L implementation\n",
    "def rouge_l(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE-L (recall, precision, and F1-score based on LCS).\n",
    "    \n",
    "    Args:\n",
    "        reference (str): The reference text.\n",
    "        candidate (str): The candidate text generated by the model.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Recall, precision, and F1-score for ROUGE-L.\n",
    "    \"\"\"\n",
    "    ref_tokens = reference.lower().split()\n",
    "    cand_tokens = candidate.lower().split()\n",
    "    lcs_length = lcs(ref_tokens, cand_tokens)\n",
    "\n",
    "    recall = lcs_length / len(ref_tokens) if len(ref_tokens) > 0 else 0.0\n",
    "    precision = lcs_length / len(cand_tokens) if len(cand_tokens) > 0 else 0.0\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1_score\": f1_score\n",
    "    }\n",
    "\n",
    "\n",
    "#     # Example reference and candidate texts\n",
    "#     reference_text = \"The cat sat on the mat and looked at the door.\"\n",
    "#     candidate_text = \"The cat sat on the mat by the door.\"\n",
    "\n",
    "#     # Compute ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "#     rouge1 = rouge_n(reference_text, candidate_text, n=1)  # ROUGE-1 (unigrams)\n",
    "#     rouge2 = rouge_n(reference_text, candidate_text, n=2)  # ROUGE-2 (bigrams)\n",
    "#     rouge_l_result = rouge_l(reference_text, candidate_text)  # ROUGE-L (LCS)\n",
    "\n",
    "#     # Print results\n",
    "#     print(\"ROUGE-1:\", rouge1)\n",
    "#     print(\"ROUGE-2:\", rouge2)\n",
    "#     print(\"ROUGE-L:\", rouge_l_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f72897be-8f0e-4504-9182-60d8b84c8a8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Reference Answer</th>\n",
       "      <th>LLM Answer</th>\n",
       "      <th>Jaccard Index</th>\n",
       "      <th>Dice Coefficient</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>The Virgin Mary allegedly appeared to a young ...</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.272166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>In front of the Notre Dame Main Building, you ...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.191180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>The Basilica of the Sacred Heart at Notre Dame...</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.350070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>The Grotto at Notre Dame, officially known as ...</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.223782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>The Main Building at the University of Notre D...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.635085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                          Reference Answer  \\\n",
       "0               Saint Bernadette Soubirous   \n",
       "1                a copper statue of Christ   \n",
       "2                        the Main Building   \n",
       "3  a Marian place of prayer and reflection   \n",
       "4       a golden statue of the Virgin Mary   \n",
       "\n",
       "                                          LLM Answer  Jaccard Index  \\\n",
       "0  The Virgin Mary allegedly appeared to a young ...       0.117647   \n",
       "1  In front of the Notre Dame Main Building, you ...       0.066667   \n",
       "2  The Basilica of the Sacred Heart at Notre Dame...       0.045455   \n",
       "3  The Grotto at Notre Dame, officially known as ...       0.056604   \n",
       "4  The Main Building at the University of Notre D...       0.250000   \n",
       "\n",
       "   Dice Coefficient  Cosine Similarity  \n",
       "0          0.210526           0.272166  \n",
       "1          0.125000           0.191180  \n",
       "2          0.086957           0.350070  \n",
       "3          0.107143           0.223782  \n",
       "4          0.400000           0.635085  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087058a-9f47-4b67-ba64-5584e5de6fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b690448-7d44-4e76-bf10-2026324ca84f",
   "metadata": {},
   "source": [
    "## Calculate ROUGE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f014e97c-6abb-4bdb-baf3-0bc4645e4519",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_l_df = pd.DataFrame.from_records(\n",
    "    evaluation_df.apply(\n",
    "        lambda row: rouge_l(row['Reference Answer'], row['LLM Answer']),\n",
    "        axis=1)\n",
    ").rename(lambda c: f\"RougeL_{c}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7846680e-1766-4219-a66e-bf472b318268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RougeL_recall</th>\n",
       "      <th>RougeL_precision</th>\n",
       "      <th>RougeL_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.315789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RougeL_recall  RougeL_precision  RougeL_f1_score\n",
       "0       0.666667          0.117647         0.200000\n",
       "1       0.600000          0.047619         0.088235\n",
       "2       0.333333          0.032258         0.058824\n",
       "3       0.428571          0.038961         0.071429\n",
       "4       0.857143          0.193548         0.315789"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_l_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d4bfc9d-8637-4cd3-b9f7-81dc2aea327f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Reference Answer</th>\n",
       "      <th>LLM Answer</th>\n",
       "      <th>Jaccard Index</th>\n",
       "      <th>Dice Coefficient</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <th>RougeL_recall</th>\n",
       "      <th>RougeL_precision</th>\n",
       "      <th>RougeL_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>The Virgin Mary allegedly appeared to a young ...</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.272166</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>In front of the Notre Dame Main Building, you ...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.191180</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>The Basilica of the Sacred Heart at Notre Dame...</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.350070</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>The Grotto at Notre Dame, officially known as ...</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.223782</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>The Main Building at the University of Notre D...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.635085</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.315789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                          Reference Answer  \\\n",
       "0               Saint Bernadette Soubirous   \n",
       "1                a copper statue of Christ   \n",
       "2                        the Main Building   \n",
       "3  a Marian place of prayer and reflection   \n",
       "4       a golden statue of the Virgin Mary   \n",
       "\n",
       "                                          LLM Answer  Jaccard Index  \\\n",
       "0  The Virgin Mary allegedly appeared to a young ...       0.117647   \n",
       "1  In front of the Notre Dame Main Building, you ...       0.066667   \n",
       "2  The Basilica of the Sacred Heart at Notre Dame...       0.045455   \n",
       "3  The Grotto at Notre Dame, officially known as ...       0.056604   \n",
       "4  The Main Building at the University of Notre D...       0.250000   \n",
       "\n",
       "   Dice Coefficient  Cosine Similarity  RougeL_recall  RougeL_precision  \\\n",
       "0          0.210526           0.272166       0.666667          0.117647   \n",
       "1          0.125000           0.191180       0.600000          0.047619   \n",
       "2          0.086957           0.350070       0.333333          0.032258   \n",
       "3          0.107143           0.223782       0.428571          0.038961   \n",
       "4          0.400000           0.635085       0.857143          0.193548   \n",
       "\n",
       "   RougeL_f1_score  \n",
       "0         0.200000  \n",
       "1         0.088235  \n",
       "2         0.058824  \n",
       "3         0.071429  \n",
       "4         0.315789  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([evaluation_df, rouge_l_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ba1fc-1e07-4dc2-b8bb-fdbfa7d99aeb",
   "metadata": {},
   "source": [
    "Also create ROUGE_1, ROUGE_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee572389-969c-4382-b333-52f1299c851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_N_list = [\n",
    "    pd.DataFrame.from_records(\n",
    "        evaluation_df.apply(\n",
    "            lambda row: rouge_n(row['Reference Answer'], row['LLM Answer'], n),\n",
    "            axis=1)\n",
    "    ).rename(lambda c: f\"Rouge{n}_{c}\", axis=1)\n",
    "    for n in (1, 2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66c1adbe-7b91-43aa-b624-bad09db58674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Evaluation Table: (5, 15)\n"
     ]
    }
   ],
   "source": [
    "evaluation_df2 = pd.concat([evaluation_df, rouge_l_df] + rouge_N_list, axis=1)\n",
    "print(f\"Shape of Evaluation Table: {evaluation_df2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5938d29d-e6d6-4de4-b2cc-d5aee35d0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df2.to_csv(\"token_based_metrics_example_table2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628293c6-e21e-4a79-8efb-255be94440a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef5ccdd1-b69a-4e54-b8b9-a484c9098a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jaccard Index</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.107274</td>\n",
       "      <td>0.084434</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dice Coefficient</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.185925</td>\n",
       "      <td>0.128595</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.334456</td>\n",
       "      <td>0.178381</td>\n",
       "      <td>0.191180</td>\n",
       "      <td>0.223782</td>\n",
       "      <td>0.272166</td>\n",
       "      <td>0.350070</td>\n",
       "      <td>0.635085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RougeL_recall</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.577143</td>\n",
       "      <td>0.205215</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RougeL_precision</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.086007</td>\n",
       "      <td>0.069178</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.193548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RougeL_f1_score</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.146855</td>\n",
       "      <td>0.109816</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rouge1_recall</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.577143</td>\n",
       "      <td>0.205215</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rouge1_precision</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.086007</td>\n",
       "      <td>0.069178</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.193548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rouge1_f1_score</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.146855</td>\n",
       "      <td>0.109816</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rouge2_recall</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.298142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rouge2_precision</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.042392</td>\n",
       "      <td>0.056918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rouge2_f1_score</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.095132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count      mean       std       min       25%       50%  \\\n",
       "Jaccard Index        5.0  0.107274  0.084434  0.045455  0.056604  0.066667   \n",
       "Dice Coefficient     5.0  0.185925  0.128595  0.086957  0.107143  0.125000   \n",
       "Cosine Similarity    5.0  0.334456  0.178381  0.191180  0.223782  0.272166   \n",
       "RougeL_recall        5.0  0.577143  0.205215  0.333333  0.428571  0.600000   \n",
       "RougeL_precision     5.0  0.086007  0.069178  0.032258  0.038961  0.047619   \n",
       "RougeL_f1_score      5.0  0.146855  0.109816  0.058824  0.071429  0.088235   \n",
       "Rouge1_recall        5.0  0.577143  0.205215  0.333333  0.428571  0.600000   \n",
       "Rouge1_precision     5.0  0.086007  0.069178  0.032258  0.038961  0.047619   \n",
       "Rouge1_f1_score      5.0  0.146855  0.109816  0.058824  0.071429  0.088235   \n",
       "Rouge2_recall        5.0  0.283333  0.298142  0.000000  0.000000  0.250000   \n",
       "Rouge2_precision     5.0  0.042392  0.056918  0.000000  0.000000  0.016129   \n",
       "Rouge2_f1_score      5.0  0.072727  0.095132  0.000000  0.000000  0.030303   \n",
       "\n",
       "                        75%       max  \n",
       "Jaccard Index      0.117647  0.250000  \n",
       "Dice Coefficient   0.210526  0.400000  \n",
       "Cosine Similarity  0.350070  0.635085  \n",
       "RougeL_recall      0.666667  0.857143  \n",
       "RougeL_precision   0.117647  0.193548  \n",
       "RougeL_f1_score    0.200000  0.315789  \n",
       "Rouge1_recall      0.666667  0.857143  \n",
       "Rouge1_precision   0.117647  0.193548  \n",
       "Rouge1_f1_score    0.200000  0.315789  \n",
       "Rouge2_recall      0.500000  0.666667  \n",
       "Rouge2_precision   0.062500  0.133333  \n",
       "Rouge2_f1_score    0.111111  0.222222  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df2.describe().T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
