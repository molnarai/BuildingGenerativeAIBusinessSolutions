# Build stage for llama.cpp
FROM ubuntu:22.04 as builder

# Install build dependencies
RUN apt-get update && apt-get install -y \
    git \
    cmake \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Clone and build llama.cpp
WORKDIR /build
RUN git clone https://github.com/ggml-org/llama.cpp && \
    cd llama.cpp && \
    mkdir build && \
    cd build && \
    cmake .. && \
    cmake --build . --config Release

# Final stage using Hugging Face container as base
FROM huggingface/transformers-pytorch-gpu
# Create a directory for llama.cpp binaries
RUN mkdir -p /opt/llama.cpp
COPY --from=builder /build/llama.cpp/build /opt/llama.cpp

# Add llama.cpp binaries to PATH
ENV PATH="/opt/llama.cpp/bin:${PATH}"

# Add app 
RUN mkdir -p /myapp/
WORKDIR /myapp
RUN cd /myapp
COPY ./requirements.txt .
RUN pip3 install  -r ./requirements.txt --root-user-action ignore
COPY src .
# ENTRYPOINT [ "/bin/bash" ]
# CMD [ "pwd", ";", "ls -lR" ]
