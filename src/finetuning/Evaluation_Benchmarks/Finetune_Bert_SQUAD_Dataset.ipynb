{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acdc2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8aa2e2d",
   "metadata": {},
   "source": [
    "# Fine-tune BERT on SQuAD\n",
    "\n",
    "Source: \n",
    "\n",
    "https://github.com/dpoulopoulos/bert-qa-finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0896d35",
   "metadata": {},
   "source": [
    "## BERT for Question-Answering\n",
    "\n",
    "In this Notebook, we fine-tune [BERT (Bidirectional Encoder Representations from Transformers)](https://arxiv.org/abs/1810.04805) for Question Answering (Q&A) tasks using the [SQuAD (Stanford Question Answering)](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/Super_Bowl_50.html) dataset. Developed by Google in 2018, BERT revolutionized the field of NLP by setting new state-of-the-art benchmarks across various NLP (Natural Language Processing) tasks.\n",
    "\n",
    "BERT is pre-trained on a massive corpus, allowing it to grasp language structure and context. This pre-trained model can then be fine-tuned for specific tasks such as sentiment analysis or question answering. Fine-tuning BERT for Q&A tasks involves adjusting the model to predict the start and end positions of the answer in a given passage for a provided question (extractive question answering). The following steps outline the process of fine-tuning BERT for these tasks:\n",
    "\n",
    "1. **ðŸŒ± Dataset Preparation**:\n",
    "    - Define each dataset item with a question, a passage (or context), and the start and end positions of the answer within the passage as the label.\n",
    "    - Tokenize both the question and passage into subwords using BERT's tokenizer. Separate the question from the passage using the `[SEP]` token and start the input sequence with the `[CLS]`\n",
    "      token.\n",
    "    - Mark the question as segment `A` (or `0`) and the context as segment `B` (or `1`). Use this information to learn different embeddings for each segment, which are added to the word\n",
    "      embeddings.\n",
    "1. **ðŸª¡ Model Modification**:\n",
    "    - Extract embeddings for each token in the sequence from the pre-trained BERT model.\n",
    "    - Add a dense (fully connected) layer on top of BERT, with two output nodes: one for predicting the start position and one for predicting the end position of the answer in the passage (see\n",
    "      [code](https://github.com/huggingface/transformers/blob/c385de24414e4ec6125ee14c46c128bfe70ecb66/src/transformers/models/bert/modeling_bert.py#L1803)).\n",
    "1. **ðŸŽ¯ Training Objective**:\n",
    "    - Output a score for each token in the passage, indicating how likely that token is the start of the answer, and another score for the end.\n",
    "    - Apply a SoftMax function over the sequence to get a probability distribution for the start and end positions.\n",
    "    - Use the sum of the negative log likelihood of the correct start and end positions as the loss function.\n",
    "1. **ðŸš€ Training**:\n",
    "    - Initialize training with pre-trained BERT weights.\n",
    "    - Apply a smaller learning rate (e.g., 2e-5 or 3e-5) since BERT is already pre-trained. Avoid using a larger learning rate, as it may cause the model to diverge.\n",
    "    - Fine-tune the model on the Q&A dataset for several epochs, stopping when validation performance plateaus or decreases.\n",
    "1. **âœ¨ Inference**:\n",
    "    - Tokenize a new question and passage, and add the special `[CLS]` and `[SEP]` tokens.\n",
    "    - Feed the tokens into the fine-tuned BERT model to get scores for the start and end positions of the answer.\n",
    "    - Select the tokens between the predicted start and end positions as the final answer.\n",
    "    - Apply constraints such as ensuring the end position is after the start and limiting the maximum answer length.\n",
    "1. **ðŸŽ‰ Evaluation**:\n",
    "    - Measure performance using common metrics like Exact Match (EM), which calculates the percentage of predictions that exactly match the ground truth and the F1 score to account for partial\n",
    "      matches by considering overlapping words between the prediction and ground truth.\n",
    "\n",
    "Let's begin by implementing each step, one by one, using the Hugging Face ðŸ¤— ecosystem. First, the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b84e089",
   "metadata": {
    "id": "a1c533ab-4907-4397-a4bd-11c8ac50fbd1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from functools import partial\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81e909",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "- **Model Selection:** I am using Hugging Face's ecosystem to fine-tune the pre-trained BERT model (bert-base-uncased). The model was fine-tuned using the SQuAD dataset.\n",
    "\n",
    "- **Evaluation:** The model was evaluated using metrics such as exact match (EM) and F1 score to assess how well it identified the correct span of the answer from the context.\n",
    "\n",
    "- **SQuAD:** \n",
    "\n",
    "    - Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
    "\n",
    "    - SQuAD 1.1, the previous version of the SQuAD dataset, contains 100,000+ question-answer pairs on 500+ articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b8587e",
   "metadata": {},
   "source": [
    "### Access SQuAD Datset via Hugging Face Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f5b41",
   "metadata": {
    "id": "df57034d-bc42-472f-abfd-04a797218141"
   },
   "source": [
    "Let's set the IDs for the model and the dataset. We will download both of them from the Hugging Face Hub, using the `datasets` and `transformers` libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcc92326",
   "metadata": {
    "id": "9627ab22-efd5-4270-9011-547028913250"
   },
   "outputs": [],
   "source": [
    "DATASET_ID = \"rajpurkar/squad\"\n",
    "MODEL_ID = \"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4998ec0e",
   "metadata": {
    "id": "cce0e9e5-194c-40bd-bbc0-d11e917c3de3"
   },
   "source": [
    "### Data Processing\n",
    "\n",
    "In this section, we will download the dataset, cache it locally, and preprocess it into the format described in the introduction. Our goal is to produce examples that contain:\n",
    "- The tokenized question and context.\n",
    "- The start position of the answer within the context.\n",
    "- The end position of the answer within the context.\n",
    "\n",
    "First, let's download and examine the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe6b724",
   "metadata": {
    "id": "ba00863f-6db4-40ae-8a60-19abba7b244a",
    "outputId": "f13153ea-9c4a-45e2-9388-ed75eaea27d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87599/87599 [00:00<00:00, 576945.59 examples/s]\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10570/10570 [00:00<00:00, 423282.79 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the SQuAD dataset\n",
    "data = load_dataset(DATASET_ID)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0967c",
   "metadata": {
    "id": "27716155-43da-44f4-a04a-c4099d5f571b"
   },
   "source": [
    "The dataset has two splits: a `train` split with `87,599` rows, and a `validation` split with `10,570` rows. Each row includes a unique identifier, a title, the context, the question, and one or more possible answers. We will handle each split slightly differently. Youâ€™ll see why later, but for now, let's focus on processing the `train` split.\n",
    "\n",
    "Since we need to tokenize the sequences, let's begin by loading the BERT tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5bf7eb3",
   "metadata": {
    "id": "6500541f-d9a1-405d-89e3-9eaf645aad6b"
   },
   "outputs": [],
   "source": [
    "# load the BERT tokenizer\n",
    "# set `clean_up_tokenization_spaces` to False to keep the tokenization spaces\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa9a23",
   "metadata": {
    "id": "48b7f09c-48f9-4196-83df-4b2fedc77ea9"
   },
   "source": [
    "Next, let's structure the examples in the desired format. First, we tokenize the questions and the context. Then, using the answer and sequence IDs (or segment IDs, as mentioned in the introduction), we identify the start and end positions of the answer within the context. Finally, we will apply the preprocessing function to each row in the `train` set and discard the columns we don't need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb70eda",
   "metadata": {
    "id": "d122e349-613c-4353-9896-856f15daf4ff"
   },
   "outputs": [],
   "source": [
    "def preprocess_train_examples(examples, tokenizer, max_length, stride):\n",
    "    \"\"\"Process the training split of the SQuAD dataset.\n",
    "\n",
    "    Process the training split of the SQuAD dataset to include tokenized questions\n",
    "    and context, as well as the start and end positions of the answer within the context.\n",
    "\n",
    "    Args:\n",
    "        examples: A row from the dataset containing an example.\n",
    "        tokenizer: The BERT tokenizer to be used.\n",
    "        max_length: The maximum length of the input sequence. If exceeded, truncate the second\n",
    "            sentence of a pair (or a batch of pairs) to fit within the limit.\n",
    "        stride: The number of tokens to retain from the end of a truncated sequence, allowing\n",
    "            for overlap between truncated and overflowing sequences.\n",
    "\n",
    "    Returns:\n",
    "        The processed example.\n",
    "    \"\"\"\n",
    "    # Tokenize the questions and context sequences\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "      questions,\n",
    "      examples[\"context\"],\n",
    "      truncation=\"only_second\",\n",
    "      padding=\"max_length\",\n",
    "      stride=stride,\n",
    "      max_length=max_length,\n",
    "      return_offsets_mapping=True,\n",
    "      return_overflowing_tokens=True,\n",
    "    )\n",
    "\n",
    "    answers = examples[\"answers\"]\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # find the start and end positions of the answer within the context\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # if the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fa4b631",
   "metadata": {
    "id": "66c25935-e71a-4058-8381-228c7d1c07bf",
    "outputId": "3a15d334-707d-49a4-a2fb-76465fff7c85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87599/87599 [00:25<00:00, 3378.06 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 88524\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_train_data = partial(\n",
    "    preprocess_train_examples, tokenizer=tokenizer, max_length=384, stride=128)\n",
    "processed_train_data = data[\"train\"].map(preprocess_train_data, batched=True, remove_columns=data[\"train\"].column_names)\n",
    "processed_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c1510",
   "metadata": {
    "id": "66529e00-d6ce-4718-8ea4-983c13afbc0a"
   },
   "source": [
    "The processed `train` split is now ready for fine-tuning BERT. Moving on to model evaluation, the preprocessing step for the `validation` split is almost identical. However, we also need to retain the ID of each row so that we can later evaluate the model's performance by reconstructing the actual answer text and computing the Exact Match (EM) and F1 scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92e7d490",
   "metadata": {
    "id": "aae4bcf1-db3e-4162-88fc-ee04753ec455"
   },
   "outputs": [],
   "source": [
    "def preprocess_valid_examples(examples, tokenizer, max_length, stride):\n",
    "    \"\"\"Process the validation split of the SQuAD dataset.\n",
    "\n",
    "    Process the training split of the SQuAD dataset to include the unique ID of each row,\n",
    "    the tokenized questions and context, as well as the start and end positions of the answer\n",
    "    within the context.\n",
    "\n",
    "    Args:\n",
    "        examples: A row from the dataset containing an example.\n",
    "        tokenizer: The BERT tokenizer to be used.\n",
    "        max_length: The maximum length of the input sequence. If exceeded, truncate the second\n",
    "            sentence of a pair (or a batch of pairs) to fit within the limit.\n",
    "        stride: The number of tokens to retain from the end of a truncated sequence, allowing\n",
    "            for overlap between truncated and overflowing sequences.\n",
    "\n",
    "    Returns:\n",
    "        The processed example.\n",
    "    \"\"\"\n",
    "    # Tokenize the questions and context sequences\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "      questions,\n",
    "      examples[\"context\"],\n",
    "      truncation=\"only_second\",\n",
    "      padding=\"max_length\",\n",
    "      stride=stride,\n",
    "      max_length=max_length,\n",
    "      return_offsets_mapping=True,\n",
    "      return_overflowing_tokens=True,\n",
    "    )\n",
    "\n",
    "    example_ids = []\n",
    "    answers = examples[\"answers\"]\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # find the start and end positions of the answer within the context\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # if the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids  # keep the unique ID of the example\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b199ee",
   "metadata": {
    "id": "81c1c7ba-151d-48cd-9988-fd8fc8132a43",
    "outputId": "e61e21b7-2a5c-435f-89ed-5fc101623dd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10570/10570 [00:04<00:00, 2425.76 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id', 'start_positions', 'end_positions'],\n",
       "    num_rows: 10784\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_valid_data = partial(\n",
    "    preprocess_valid_examples, tokenizer=tokenizer, max_length=384, stride=128)\n",
    "processed_valid_data = data[\"validation\"].map(preprocess_valid_data, batched=True, remove_columns=data[\"validation\"].column_names)\n",
    "processed_valid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b14a1",
   "metadata": {
    "id": "ff3d4459-fe34-42de-bad1-ce4b48b61139"
   },
   "source": [
    "### Model Fine-Tuning\n",
    "\n",
    "We are now ready to fine-tune BERT for Question Answering. First, let's load the model and set the training arguments. Specifically:\n",
    "\n",
    "- We will save a model checkpoint every 2000 steps.\n",
    "- We will log the training process every 500 steps, allowing us to visualize it with TensorBoard and evaluate the experiment's performance.\n",
    "- We will use mixed precision training by casting the model weights to `bf16` (Brain Float) to accelerate the process and reduce the model's memory footprint on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa45043a",
   "metadata": {
    "id": "470c2f1b-0ae0-40f2-9ea3-0145f41a81bc",
    "outputId": "70bddf9e-d846-4b7d-9dcb-e316f9e84036"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd77e5ed",
   "metadata": {
    "id": "1cfecbba-59a9-4c07-8b27-f8610fe9acef"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./checkpoints',\n",
    "    logging_dir='./logs',\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    save_steps=2000,\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=48,\n",
    "    per_device_eval_batch_size=96,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a7a44eb",
   "metadata": {
    "id": "2155825b-0a50-487a-b8ba-dc5400330921"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_536/2208022166.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_train_data,\n",
    "    eval_dataset=processed_valid_data,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ad39b",
   "metadata": {
    "id": "03b2ddee-9562-4bac-9e45-49fecd236437"
   },
   "source": [
    "However, before we start fine-tuning the model, let's assess its performance using the Exact Match (EM) and F1 scores, as well as by answering a few sample questions from the validation split of the dataset. To evaluate the model, we need to perform some post-processing to reconstruct the actual answer from the modelâ€™s predictions and compare it to the ground truth answers. The following function handles that process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aeec985",
   "metadata": {
    "id": "8a784484-7182-49c9-a3dc-47479c99ec1d"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(start_logits, end_logits, features, examples, n_best=20, max_answer_length=50):\n",
    "    \"\"\"Compute the Exact Match (EM) and F1 score for the model's predictions.\n",
    "\n",
    "    Reconstruct the actual text of the answer from the model's predictions and compare\n",
    "    it to the ground truth for the validation dataset.\n",
    "\n",
    "    Args:\n",
    "        start_logits: Logits predicting the start position of the answer.\n",
    "        end_logits: Logits predicting the end position of the answer.\n",
    "        features: The processed validation dataset.\n",
    "        examples: The raw validation dataset.\n",
    "        n_best: The top-k answers to consider.\n",
    "        max_answer_length: The maximum length of an answer to consider.\n",
    "\n",
    "    Returns:\n",
    "        The Exact Match (EM) and F1 score for the validation dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    metric = evaluate.load(\"squad\")\n",
    "\n",
    "    # keep a dictionary that maps examples to predictions through unique IDs\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # keep a list of the top-k best predictions for the start and end position indexes\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    # reconstruct the answer considering each prediction for the start and end positions\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # select the answer with the best score based on the logit scores\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            # create a list with the predictions that contains the IDs and actual text\n",
    "            # see: https://huggingface.co/spaces/evaluate-metric/squad\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    # create a list with the labels that contains the IDs and actual text\n",
    "    # see: https://huggingface.co/spaces/evaluate-metric/squad\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a22c089",
   "metadata": {
    "id": "92fd78e1-f9f8-4e6c-94c9-5c252cfb4f00"
   },
   "source": [
    "Next, let's get the untrained model's predictions and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "508ff99d",
   "metadata": {
    "id": "2a2900c9-deb2-4ce1-92ed-83083fa6e228",
    "outputId": "c39ae703-c179-4426-e025-3ae285defdaf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.53k/4.53k [00:00<00:00, 9.11MB/s]\n",
      "Downloading extra modules: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.32k/3.32k [00:00<00:00, 8.46MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10570/10570 [00:09<00:00, 1116.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.12298959318826869, 'f1': 7.388432885067443}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, _, _ = trainer.predict(processed_valid_data)\n",
    "start_logits, end_logits = predictions\n",
    "compute_metrics(start_logits, end_logits, processed_valid_data, data[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3545353",
   "metadata": {
    "id": "1c3c4a34-941a-4913-96fa-0603562db656"
   },
   "source": [
    "As expected, the computed scores show that the model is randomly extracting text from the context to formulate answers. Letâ€™s observe this behavior in action by answering a few random questions from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd43bbee",
   "metadata": {
    "id": "9c57d8bd-c894-4582-a313-18357c543e84",
    "outputId": "dd50140b-d2f2-4d48-85d6-05bc868344e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      "\n",
      " During Reconstruction and the Gilded Age, Jacksonville and nearby St. Augustine became popular winter resorts for the rich and famous. Visitors arrived by steamboat and later by railroad. President Grover Cleveland attended the Sub-Tropical Exposition in the city on February 22, 1888 during his trip to Florida. This highlighted the visibility of the state as a worthy place for tourism. The city's tourism, however, was dealt major blows in the late 19th century by yellow fever outbreaks. In addition, extension of the Florida East Coast Railway further south drew visitors to other areas. From 1893 to 1938 Jacksonville was the site of the Florida Old Confederate Soldiers and Sailors Home with a nearby cemetery. \n",
      "\n",
      "Question: \n",
      "\n",
      " Which US President visited Jacksonville in 1888? \n",
      "\n",
      "Answer: \n",
      "\n",
      " city's tourism, however, was dealt major blows in the \n",
      "\n",
      "--- \n",
      "\n",
      "Context: \n",
      "\n",
      " Several commemorative events take place every year. Gatherings of thousands of people on the banks of the Vistula on Midsummerâ€™s Night for a festival called Wianki (Polish for Wreaths) have become a tradition and a yearly event in the programme of cultural events in Warsaw. The festival traces its roots to a peaceful pagan ritual where maidens would float their wreaths of herbs on the water to predict when they would be married, and to whom. By the 19th century this tradition had become a festive event, and it continues today. The city council organize concerts and other events. Each Midsummerâ€™s Eve, apart from the official floating of wreaths, jumping over fires, looking for the fern flower, there are musical performances, dignitaries' speeches, fairs and fireworks by the river bank. \n",
      "\n",
      "Question: \n",
      "\n",
      " How man people gather along the banks of the Vistula for the Wianki festival? \n",
      "\n",
      "Answer: \n",
      "\n",
      " the banks of the Vistula on Midsummer \n",
      "\n",
      "--- \n",
      "\n",
      "Context: \n",
      "\n",
      " NE1fm launched on 8 June 2007, the first full-time community radio station in the area. Newcastle Student Radio is run by students from both of the city's universities, broadcasting from Newcastle University's student's union building during term time. Radio Tyneside has been the voluntary hospital radio service for most hospitals across Newcastle and Gateshead since 1951, broadcasting on Hospedia  and online. The city also has a Radio Lollipop station based at the Great North Children's Hospital in the Newcastle Royal Victoria Infirmary. \n",
      "\n",
      "Question: \n",
      "\n",
      " Where does the Newcastle Student Radio station broadcast from during terms? \n",
      "\n",
      "Answer: \n",
      "\n",
      " union building during \n",
      "\n",
      "--- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_indexes = np.random.randint(0, len(data[\"validation\"]), 3)\n",
    "subdataset = data[\"validation\"].select(random_indexes)\n",
    "qa_pipe_untrained = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device='cuda')\n",
    "\n",
    "for row in subdataset:\n",
    "    context = row[\"context\"]\n",
    "    question = row[\"question\"]\n",
    "    answer = qa_pipe_untrained(question=question, context=context)\n",
    "\n",
    "    print(f\"Context: \\n\\n {context} \\n\")\n",
    "    print(f\"Question: \\n\\n {question} \\n\")\n",
    "    print(f\"Answer: \\n\\n {answer['answer']} \\n\")\n",
    "    print(\"--- \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d07c30a",
   "metadata": {
    "id": "feee0071-2e17-4046-978e-f292bc25fb3d"
   },
   "source": [
    "Let's fine-tune BERT so it gives better answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cf5a82f",
   "metadata": {
    "id": "789143f0-952e-420b-b0cb-f535a49d1332",
    "outputId": "422a0655-b5cd-4e17-ed65-e9ae50e1109f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3690' max='3690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3690/3690 14:04, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.901500</td>\n",
       "      <td>1.236201</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.246300</td>\n",
       "      <td>1.101290</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.135700</td>\n",
       "      <td>1.049315</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.003900</td>\n",
       "      <td>1.049722</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.842400</td>\n",
       "      <td>1.028441</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.830300</td>\n",
       "      <td>1.024863</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.825400</td>\n",
       "      <td>1.016140</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3690, training_loss=1.0966898848370807, metrics={'train_runtime': 844.5176, 'train_samples_per_second': 209.644, 'train_steps_per_second': 4.369, 'total_flos': 3.4696551139946496e+16, 'train_loss': 1.0966898848370807, 'epoch': 2.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecafaa4",
   "metadata": {
    "id": "81b0a399-ae59-408d-b67b-85c0e7041802"
   },
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Finally, we need to evaluate the model on the `validation` split of the dataset. We will use two metrics to systematically assess its performance:\n",
    "- **Exact Match (EM)**: Calculate the percentage of predictions that exactly match the ground truth.\n",
    "- **F1 Score**: Measure partial matches by considering overlapping words between the prediction and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0133c4c",
   "metadata": {
    "id": "b0284cf6-0dc9-4f4a-9f4c-ea31f2fcecb0",
    "outputId": "f6dbcfe8-789a-4ded-e7c2-6bf52cf0b1b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10570/10570 [00:09<00:00, 1071.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 79.80132450331126, 'f1': 87.53663335317574}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, _, _ = trainer.predict(processed_valid_data)\n",
    "start_logits, end_logits = predictions\n",
    "compute_metrics(start_logits, end_logits, processed_valid_data, data[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d9f777",
   "metadata": {
    "id": "b82c8c9a-51ff-4b0a-b7a1-4f722cdd4d8c"
   },
   "source": [
    "Let's also provide an answer for the same random samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5532c805",
   "metadata": {
    "id": "bc5f9150-bbfc-4779-bb8e-08b030ff25c0",
    "outputId": "1de1c0a1-055f-4a20-f351-941883ad5666"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      "\n",
      " During Reconstruction and the Gilded Age, Jacksonville and nearby St. Augustine became popular winter resorts for the rich and famous. Visitors arrived by steamboat and later by railroad. President Grover Cleveland attended the Sub-Tropical Exposition in the city on February 22, 1888 during his trip to Florida. This highlighted the visibility of the state as a worthy place for tourism. The city's tourism, however, was dealt major blows in the late 19th century by yellow fever outbreaks. In addition, extension of the Florida East Coast Railway further south drew visitors to other areas. From 1893 to 1938 Jacksonville was the site of the Florida Old Confederate Soldiers and Sailors Home with a nearby cemetery. \n",
      "\n",
      "Question: \n",
      "\n",
      " Which US President visited Jacksonville in 1888? \n",
      "\n",
      "Answer: \n",
      "\n",
      " Grover Cleveland \n",
      "\n",
      "--- \n",
      "\n",
      "Context: \n",
      "\n",
      " Several commemorative events take place every year. Gatherings of thousands of people on the banks of the Vistula on Midsummerâ€™s Night for a festival called Wianki (Polish for Wreaths) have become a tradition and a yearly event in the programme of cultural events in Warsaw. The festival traces its roots to a peaceful pagan ritual where maidens would float their wreaths of herbs on the water to predict when they would be married, and to whom. By the 19th century this tradition had become a festive event, and it continues today. The city council organize concerts and other events. Each Midsummerâ€™s Eve, apart from the official floating of wreaths, jumping over fires, looking for the fern flower, there are musical performances, dignitaries' speeches, fairs and fireworks by the river bank. \n",
      "\n",
      "Question: \n",
      "\n",
      " How man people gather along the banks of the Vistula for the Wianki festival? \n",
      "\n",
      "Answer: \n",
      "\n",
      " thousands \n",
      "\n",
      "--- \n",
      "\n",
      "Context: \n",
      "\n",
      " NE1fm launched on 8 June 2007, the first full-time community radio station in the area. Newcastle Student Radio is run by students from both of the city's universities, broadcasting from Newcastle University's student's union building during term time. Radio Tyneside has been the voluntary hospital radio service for most hospitals across Newcastle and Gateshead since 1951, broadcasting on Hospedia  and online. The city also has a Radio Lollipop station based at the Great North Children's Hospital in the Newcastle Royal Victoria Infirmary. \n",
      "\n",
      "Question: \n",
      "\n",
      " Where does the Newcastle Student Radio station broadcast from during terms? \n",
      "\n",
      "Answer: \n",
      "\n",
      " Newcastle University's student's union building \n",
      "\n",
      "--- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device='cuda')\n",
    "\n",
    "for row in subdataset:\n",
    "    context = row[\"context\"]\n",
    "    question = row[\"question\"]\n",
    "    answer = qa_pipe(question=question, context=context)\n",
    "\n",
    "    print(f\"Context: \\n\\n {context} \\n\")\n",
    "    print(f\"Question: \\n\\n {question} \\n\")\n",
    "    print(f\"Answer: \\n\\n {answer['answer']} \\n\")\n",
    "    print(\"--- \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e7c732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02badcde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
