{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your AI Agents using Python, HuggingFace Models and Semantic Kernel\n",
    "\n",
    "Source: \n",
    "\n",
    "Dr. Nimrita Koul, Building your AI Agents using Python, HuggingFace Models and Semantic Kernel, https://medium.com/@nimritakoul01/building-your-ai-agents-using-python-huggingface-models-and-semantic-kernel-a2242432da30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install semantic-kernel==0.9.6b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers sentence_transformers accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion, HuggingFaceTextEmbedding\n",
    "from semantic_kernel.core_plugins import TextMemoryPlugin\n",
    "from semantic_kernel.memory import SemanticTextMemory, VolatileMemoryStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Semantic Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8032a8e4acc425fa30c4559b42ae547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09d84cbf3744e719ff9a2b4e5ce756c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c68acd62174a5abd78e83688b25b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633b9d2dadc04d89983dd7e337e9177e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0897ca74e8634c7f964a1305d183afa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c179b70f141e473490d97318df8c5b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92b1f0810d84903904e107cb97240d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1229fb9c4fc44c2ab85327bc4035dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a943db307dd14dadabee9c98a28d905c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a9c517b25e41079650fd703c9dad9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502f849856c042b5ae58be0b29ca1f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44297ee9e32345a3aa76cc243da07f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e25fbcb5f734392875f00ecfbcb084e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78af52120234ae6bf6a41b302722a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af330c4d3f6544dc9d054cc938987e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e4ac7617b847cca052a7886a86e734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a700cf4ccde44cebacd306acb5a5ab7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eec585e8aa240de83f49e54a657b00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='TextMemoryPlugin', description=None, functions={'recall': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='recall', plugin_name='TextMemoryPlugin', description='Recall a fact from the long term memory', parameters=[KernelParameterMetadata(name='ask', description='The information to retrieve', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The information to retrieve'}, include_in_function_choices=True), KernelParameterMetadata(name='collection', description='The collection to search for information.', default_value='generic', type_='str', is_required=False, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The collection to search for information.'}, include_in_function_choices=True), KernelParameterMetadata(name='relevance', description='The relevance score, from 0.0 to 1.0; 1.0 means perfect match', default_value=0.75, type_='float', is_required=False, type_object=<class 'float'>, schema_data={'type': 'number', 'description': 'The relevance score, from 0.0 to 1.0; 1.0 means perfect match'}, include_in_function_choices=True), KernelParameterMetadata(name='limit', description='The maximum number of relevant memories to recall.', default_value=1, type_='int', is_required=False, type_object=<class 'int'>, schema_data={'type': 'integer', 'description': 'The maximum number of relevant memories to recall.'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x32e391490>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x337202d10>, method=<bound method TextMemoryPlugin.recall of TextMemoryPlugin(memory=SemanticTextMemory(), embeddings_kwargs={})>, stream_method=None), 'save': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='save', plugin_name='TextMemoryPlugin', description='Save information to semantic memory', parameters=[KernelParameterMetadata(name='text', description='The information to save.', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The information to save.'}, include_in_function_choices=True), KernelParameterMetadata(name='key', description='The unique key to associate with the information.', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The unique key to associate with the information.'}, include_in_function_choices=True), KernelParameterMetadata(name='collection', description='The collection to save the information.', default_value='generic', type_='str', is_required=False, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The collection to save the information.'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='None', is_required=False, type_object=None, schema_data={'type': 'object'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x16fa87090>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x338189cd0>, method=<bound method TextMemoryPlugin.save of TextMemoryPlugin(memory=SemanticTextMemory(), embeddings_kwargs={})>, stream_method=None)})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = Kernel()#create a Kernel object\n",
    "text_service_id = \"openai-community/gpt2\" #specify the LLM to use for text generation\n",
    "\n",
    "#Let us add this LLM to our kernel object\n",
    "#Since we are using Hugging Face model, we have imported and will use HuggingFaceTextCompletion class\n",
    "#Below we have added text generation model to our kernel\n",
    "kernel.add_service(\n",
    "  service=HuggingFaceTextCompletion(\n",
    "      service_id=text_service_id, ai_model_id=text_service_id, task=\"text-generation\"\n",
    "        ),\n",
    "    )\n",
    "#Next we have added an embedding model from HF to our kernel\n",
    "embed_service_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_svc = HuggingFaceTextEmbedding(service_id=embed_service_id, ai_model_id=embed_service_id)\n",
    "kernel.add_service(\n",
    "        service=embedding_svc,\n",
    "    )\n",
    "#Next we are adding volatile memory plugin to our kernel\n",
    "memory = SemanticTextMemory(storage=VolatileMemoryStore(), embeddings_generator=embedding_svc)\n",
    "kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFacePromptExecutionSettings\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "\n",
    "#let us create a collection to store 5 pieces of information in memory plugin\n",
    "#this is infomration about 5 animals\n",
    "collection_id = \"generic\"\n",
    "await memory.save_information(collection=collection_id, id=\"info1\", text=\"Sharks are fish.\")\n",
    "await memory.save_information(collection=collection_id, id=\"info2\", text=\"Whales are mammals.\")\n",
    "await memory.save_information(collection=collection_id, id=\"info3\", text=\"Penguins are birds.\")\n",
    "await memory.save_information(collection=collection_id, id=\"info4\", text=\"Dolphins are mammals.\")\n",
    "await memory.save_information(collection=collection_id, id=\"info5\", text=\"Flies are insects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Prompt Template:\n",
    "\n",
    "Prompt template asks the LLM to recall the information about animals stored in Text Memory Plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt function using SK prompt template language\n",
    "my_prompt = \"\"\"I know these animal facts:\n",
    "- {{recall 'fact about sharks'}}\n",
    "- {{recall 'fact about whales'}}\n",
    "- {{recall 'fact about penguins'}}\n",
    "- {{recall 'fact about dolphins'}}\n",
    "- {{recall 'fact about flies'}}\n",
    "Now, tell me something about: {{$request}}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup execution settings for the kernel\n",
    "\n",
    "- Set LLM model and its configuration settings\n",
    "- Set prompt template configuation\n",
    "- Create semantic function called `my_function` \n",
    "- Add it to the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execution settings for AI model\n",
    "execution_settings = HuggingFacePromptExecutionSettings(\n",
    "    service_id=text_service_id,\n",
    "    ai_model_id=text_service_id,\n",
    "    max_tokens=200,\n",
    "    eos_token_id=2,  \n",
    "    pad_token_id=0, \n",
    "    max_new_tokens = 100,\n",
    ")\n",
    "\n",
    "#prompt template configurations\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=my_prompt,\n",
    "    name=\"text_complete\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "#let the semantic function to the kernel\n",
    "# this function uses above prompt and model\n",
    "my_function = kernel.add_function(\n",
    "    function_name=\"text_complete\",\n",
    "    plugin_name=\"TextCompletionPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke function using kernel object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await kernel.invoke(\n",
    "    my_function,\n",
    "    request=\"What are whales?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The queried result for 'What are sharks?' is Sharks are fish.\n",
      "openai-community/gpt2 completed prompt with: 'I know these animal facts:\n",
      "- Sharks are fish.\n",
      "- Whales are mammals.\n",
      "- Penguins are birds.\n",
      "- Dolphins are mammals.\n",
      "- Flies are insects.\n",
      "Now, tell me something about: What are whales? Can whales have a head that makes them look like whales? Are dolphins, birds or any other species of whales that people can even identify as whale species? And who are the whales? What are they all about? The fact is that there are many different groups of animal that all share these traits. In general, they are all genetically separate. These distinctions are a result of the interbreeding between different groups of animals and of nature's natural laws. One of some of them is a single type of'\n"
     ]
    }
   ],
   "source": [
    "output = str(output).strip()\n",
    "query_result1 = await memory.search(\n",
    "    collection=collection_id, query=\"What are sharks?\", limit=1, min_relevance_score=0.3\n",
    ")\n",
    "print(f\"The queried result for 'What are sharks?' is {query_result1[0].text}\")\n",
    "print(f\"{text_service_id} completed prompt with: '{output}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a different prompt for a chat conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://github.com/microsoft/semantic-kernel/blob/main/python/notebooks/04-kernel-arguments-chat.ipynb\n",
    "prompt = \"\"\"\n",
    "ChatBot can have a conversation with you about any topic.\n",
    "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
    "{{$history}}\n",
    "User: {{$user_input}}\n",
    "ChatBot: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create prompt template configuration and a semantic function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: #https://github.com/microsoft/semantic-kernel/blob/main/python/notebooks/04-kernel-arguments-chat.ipynb\n",
    "#{{$user_input}} is used as a variable to hold user input\n",
    "#{{$history}} is used as a variable to hold previous conversation history\n",
    "\n",
    "#Register your semantic function\n",
    "from semantic_kernel.prompt_template.input_variable import InputVariable\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "ChatBot can have a conversation with you about any topic.\n",
    "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
    "\\n\n",
    "{{$history}}\n",
    "\\n\n",
    "User: {{$user_input}}\n",
    "\\n\n",
    "ChatBot: \"\"\"\n",
    "\n",
    "execution_settings = HuggingFacePromptExecutionSettings(\n",
    "    service_id=text_service_id,\n",
    "    ai_model_id=text_service_id,\n",
    "    max_tokens=100,\n",
    "    eos_token_id=2,  \n",
    "    pad_token_id=0, \n",
    "    max_new_tokens = 50,\n",
    ")\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"chat\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables = [\n",
    "        InputVariable(name=\"user_input\", description=\"The user input\", is_required=True),\n",
    "        InputVariable(name=\"history\", description=\"The conversation history\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    function_name=\"chat\",\n",
    "    plugin_name=\"chatPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a chat history object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.contents import ChatHistory\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_system_message(\"You are a helpful chatbot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a KernelArguments object to pass the user input to the chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.functions import KernelArguments\n",
    "arguments = KernelArguments(user_input=\"Tell me about wild animals\", history=chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChatBot can have a conversation with you about any topic.\n",
      "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
      "\n",
      "\n",
      "<chat_history><message role=\"system\"><text>You are a helpful chatbot.</text></message></chat_history>\n",
      "\n",
      "\n",
      "User: Tell me about wild animals\n",
      "\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: You gave us a real cow who lives in your backyard.\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: *no*\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "\n",
      "User: And how are your animals\n"
     ]
    }
   ],
   "source": [
    "response = await kernel.invoke(chat_function, arguments)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update history of chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the history with the output\n",
    "chat_history.add_assistant_message(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to continuously chat with chat bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep Chatting\n",
    "async def chat(input_text: str) -> None:\n",
    "    # Save new message in the context variables\n",
    "    print(f\"User: {input_text}\")\n",
    "\n",
    "    # Process the user message and get an answer\n",
    "    answer = await kernel.invoke(chat_function, KernelArguments(user_input=input_text, history=chat_history))\n",
    "\n",
    "    # Show the response\n",
    "    print(f\"ChatBot: {answer}\")\n",
    "\n",
    "    chat_history.add_user_message(input_text)\n",
    "    chat_history.add_assistant_message(str(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call chat bot again and again and return chat hisytory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I love Gorillas, what do you say?\n",
      "ChatBot: \n",
      "ChatBot can have a conversation with you about any topic.\n",
      "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
      "\n",
      "\n",
      "<chat_history><message role=\"system\"><text>You are a helpful chatbot.</text></message><message role=\"assistant\"><text>\n",
      "ChatBot can have a conversation with you about any topic.\n",
      "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
      "\n",
      "\n",
      "<chat_history><message role=\"system\"><text>You are a helpful chatbot.</text></message></chat_history>\n",
      "\n",
      "\n",
      "User: Tell me about wild animals\n",
      "\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: You gave us a real cow who lives in your backyard.\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: *no*\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "\n",
      "User: And how are your animals</text></message></chat_history>\n",
      "\n",
      "\n",
      "User: I love Gorillas, what do you say?\n",
      "\n",
      "\n",
      "ChatBot: ***********\n",
      "\n",
      "User: *no*\n",
      "\n",
      "ChatBot: ***********\n",
      "\n",
      "\n",
      "User: You used a car to drive an elephant to the zoo. Can you have one of my elephants?\n",
      "\n",
      "\n",
      "ChatBot: *********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await chat(\"I love Gorillas, what do you say?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Tell me about sea horses\n",
      "ChatBot: \n",
      "ChatBot can have a conversation with you about any topic.\n",
      "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
      "\n",
      "\n",
      "<chat_history><message role=\"system\"><text>You are a helpful chatbot.</text></message><message role=\"assistant\"><text>\n",
      "ChatBot can have a conversation with you about any topic.\n",
      "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
      "\n",
      "\n",
      "<chat_history><message role=\"system\"><text>You are a helpful chatbot.</text></message></chat_history>\n",
      "\n",
      "\n",
      "User: Tell me about wild animals\n",
      "\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: You gave us a real cow who lives in your backyard.\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: *no*\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "\n",
      "User: And how are your animals</text></message><message role=\"user\"><text>I love Gorillas, what do you say?</text></message><message role=\"assistant\"><text>\n",
      "ChatBot can have a conversation with you about any topic.\n",
      "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
      "\n",
      "\n",
      "<chat_history><message role=\"system\"><text>You are a helpful chatbot.</text></message><message role=\"assistant\"><text>\n",
      "ChatBot can have a conversation with you about any topic.\n",
      "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
      "\n",
      "\n",
      "<chat_history><message role=\"system\"><text>You are a helpful chatbot.</text></message></chat_history>\n",
      "\n",
      "\n",
      "User: Tell me about wild animals\n",
      "\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: You gave us a real cow who lives in your backyard.\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: *no*\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "\n",
      "User: And how are your animals</text></message></chat_history>\n",
      "\n",
      "\n",
      "User: I love Gorillas, what do you say?\n",
      "\n",
      "\n",
      "ChatBot: ***********\n",
      "\n",
      "User: *no*\n",
      "\n",
      "ChatBot: ***********\n",
      "\n",
      "\n",
      "User: You used a car to drive an elephant to the zoo. Can you have one of my elephants?\n",
      "\n",
      "\n",
      "ChatBot: *********\n",
      "</text></message></chat_history>\n",
      "\n",
      "\n",
      "User: Tell me about sea horses\n",
      "\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "\n",
      "User: I never used a horse, how are you, you?\n",
      "\n",
      "\n",
      "ChatBot: ***********\n",
      "\n",
      "ChatBot: ***********\n",
      "\n",
      "\n",
      "User: Can you have one\n"
     ]
    }
   ],
   "source": [
    "await chat(\"Tell me about sea horses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<chat_history><message role=\"system\"><text>You are a helpful chatbot.</text></message><message role=\"assistant\"><text>\n",
      "ChatBot can have a conversation with you about any topic.\n",
      "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
      "\n",
      "\n",
      "&lt;chat_history&gt;&lt;message role=\"system\"&gt;&lt;text&gt;You are a helpful chatbot.&lt;/text&gt;&lt;/message&gt;&lt;/chat_history&gt;\n",
      "\n",
      "\n",
      "User: Tell me about wild animals\n",
      "\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: You gave us a real cow who lives in your backyard.\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: *no*\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "\n",
      "User: And how are your animals</text></message><message role=\"user\"><text>I love Gorillas, what do you say?</text></message><message role=\"assistant\"><text>\n",
      "ChatBot can have a conversation with you about any topic.\n",
      "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
      "\n",
      "\n",
      "&lt;chat_history&gt;&lt;message role=\"system\"&gt;&lt;text&gt;You are a helpful chatbot.&lt;/text&gt;&lt;/message&gt;&lt;message role=\"assistant\"&gt;&lt;text&gt;\n",
      "ChatBot can have a conversation with you about any topic.\n",
      "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
      "\n",
      "\n",
      "&lt;chat_history&gt;&lt;message role=\"system\"&gt;&lt;text&gt;You are a helpful chatbot.&lt;/text&gt;&lt;/message&gt;&lt;/chat_history&gt;\n",
      "\n",
      "\n",
      "User: Tell me about wild animals\n",
      "\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: You gave us a real cow who lives in your backyard.\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: *no*\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "\n",
      "User: And how are your animals&lt;/text&gt;&lt;/message&gt;&lt;/chat_history&gt;\n",
      "\n",
      "\n",
      "User: I love Gorillas, what do you say?\n",
      "\n",
      "\n",
      "ChatBot: ***********\n",
      "\n",
      "User: *no*\n",
      "\n",
      "ChatBot: ***********\n",
      "\n",
      "\n",
      "User: You used a car to drive an elephant to the zoo. Can you have one of my elephants?\n",
      "\n",
      "\n",
      "ChatBot: *********\n",
      "</text></message><message role=\"user\"><text>Tell me about sea horses</text></message><message role=\"assistant\"><text>\n",
      "ChatBot can have a conversation with you about any topic.\n",
      "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
      "\n",
      "\n",
      "&lt;chat_history&gt;&lt;message role=\"system\"&gt;&lt;text&gt;You are a helpful chatbot.&lt;/text&gt;&lt;/message&gt;&lt;message role=\"assistant\"&gt;&lt;text&gt;\n",
      "ChatBot can have a conversation with you about any topic.\n",
      "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
      "\n",
      "\n",
      "&lt;chat_history&gt;&lt;message role=\"system\"&gt;&lt;text&gt;You are a helpful chatbot.&lt;/text&gt;&lt;/message&gt;&lt;/chat_history&gt;\n",
      "\n",
      "\n",
      "User: Tell me about wild animals\n",
      "\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: You gave us a real cow who lives in your backyard.\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: *no*\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "\n",
      "User: And how are your animals&lt;/text&gt;&lt;/message&gt;&lt;message role=\"user\"&gt;&lt;text&gt;I love Gorillas, what do you say?&lt;/text&gt;&lt;/message&gt;&lt;message role=\"assistant\"&gt;&lt;text&gt;\n",
      "ChatBot can have a conversation with you about any topic.\n",
      "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
      "\n",
      "\n",
      "&lt;chat_history&gt;&lt;message role=\"system\"&gt;&lt;text&gt;You are a helpful chatbot.&lt;/text&gt;&lt;/message&gt;&lt;message role=\"assistant\"&gt;&lt;text&gt;\n",
      "ChatBot can have a conversation with you about any topic.\n",
      "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
      "\n",
      "\n",
      "&lt;chat_history&gt;&lt;message role=\"system\"&gt;&lt;text&gt;You are a helpful chatbot.&lt;/text&gt;&lt;/message&gt;&lt;/chat_history&gt;\n",
      "\n",
      "\n",
      "User: Tell me about wild animals\n",
      "\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: You gave us a real cow who lives in your backyard.\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "User: *no*\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "\n",
      "User: And how are your animals&lt;/text&gt;&lt;/message&gt;&lt;/chat_history&gt;\n",
      "\n",
      "\n",
      "User: I love Gorillas, what do you say?\n",
      "\n",
      "\n",
      "ChatBot: ***********\n",
      "\n",
      "User: *no*\n",
      "\n",
      "ChatBot: ***********\n",
      "\n",
      "\n",
      "User: You used a car to drive an elephant to the zoo. Can you have one of my elephants?\n",
      "\n",
      "\n",
      "ChatBot: *********\n",
      "&lt;/text&gt;&lt;/message&gt;&lt;/chat_history&gt;\n",
      "\n",
      "\n",
      "User: Tell me about sea horses\n",
      "\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "ChatBot: **********\n",
      "\n",
      "\n",
      "User: I never used a horse, how are you, you?\n",
      "\n",
      "\n",
      "ChatBot: ***********\n",
      "\n",
      "ChatBot: ***********\n",
      "\n",
      "\n",
      "User: Can you have one</text></message></chat_history>\n"
     ]
    }
   ],
   "source": [
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your own functions to work with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Create function to generate a random number between 3 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code source: https://github.com/microsoft/semantic-kernel/blob/main/python/notebooks/08-native-function-inline.ipynb\n",
    "import random\n",
    "\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "class GenerateNumberPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate a number between 3-x.\n",
    "    \"\"\"\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Generate a random number between 3-x\",\n",
    "        name=\"GenerateNumberThreeOrHigher\",\n",
    "    )\n",
    "    def generate_number_three_or_higher(self, input: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a number between 3-<input>\n",
    "        Example:\n",
    "            \"8\" => rand(3,8)\n",
    "        Args:\n",
    "            input -- The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(random.randint(3, int(input)))\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {input}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Create a semantic function that accepts a number as {{input}} and generates that number of paragraphs in a story about two Corgis on an adventure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFacePromptExecutionSettings\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig, InputVariable\n",
    "\n",
    "prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$input}} paragraphs long. It must be this length.\n",
    "\"\"\"\n",
    "\n",
    "execution_settings = HuggingFacePromptExecutionSettings(\n",
    "    service_id=text_service_id,\n",
    "    ai_model_id=text_service_id,\n",
    "    max_tokens=300,\n",
    "    eos_token_id=2,  \n",
    "    pad_token_id=0, \n",
    "    max_new_tokens = 50,\n",
    ")\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"story\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"input\", description=\"The user input\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "corgi_story = kernel.add_function(\n",
    "    function_name=\"CorgiStory\",\n",
    "    plugin_name=\"CorgiPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "\n",
    "generate_number_plugin = kernel.add_plugin(GenerateNumberPlugin(), \"GenerateNumberPlugin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the paragraph count by running the random number generator function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "#Let's generate a paragraph count.\n",
    "# Run the number generator\n",
    "generate_number_three_or_higher = generate_number_plugin[\"GenerateNumberThreeOrHigher\"]\n",
    "number_result = await generate_number_three_or_higher(kernel, input=6)\n",
    "print(number_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the semantic function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = await corgi_story.invoke(kernel, input=number_result.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story exactly 6 paragraphs long.\n",
      "=====================================================\n",
      "\n",
      "Write a short story about two Corgis on an adventure.\n",
      "The story must be:\n",
      "- G rated\n",
      "- Have a positive message\n",
      "- No sexism, racism or other bias/bigotry\n",
      "- Be exactly 6 paragraphs long. It must be this length.\n",
      "The story must take place in or before the time limit of the Corgis and if so, the date of it's airing.\n",
      "Make sure it takes place before 3pm on any night, as it will take time to prep.\n",
      "Get\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating a corgi story exactly {number_result.value} paragraphs long.\")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "https://learn.microsoft.com/en-us/semantic-kernel/overview/?tabs=Csharp#semantic-kernel-is-at-the-center-of-the-copilot-stack\n",
    "\n",
    "https://learn.microsoft.com/en-us/semantic-kernel/agents/\n",
    "\n",
    "https://github.com/microsoft/semantic-kernel/blob/main/python/notebooks/00-getting-started.ipynb\n",
    "\n",
    "https://devblogs.microsoft.com/semantic-kernel/how-to-use-hugging-face-models-with-semantic-kernel/\n",
    "\n",
    "https://github.com/microsoft/semantic-kernel/blob/main/python/README.md\n",
    "\n",
    "https://github.com/microsoft/semantic-kernel/blob/main/python/notebooks/00-getting-started.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agenticai_frameworks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
